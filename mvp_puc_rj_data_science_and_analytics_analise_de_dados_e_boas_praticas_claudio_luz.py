# -*- coding: utf-8 -*-
"""MVP PUC-RJ - Data Science and Analytics - Analise de Dados e Boas Praticas - Claudio Luz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fmmg43PQvAndZetGESgL6SdjDbiezl1y

<a href="https://colab.research.google.com/github/dipucriodigital/ciencia-de-dados-e-analytics/blob/main/mvp-analise-de-dados-e-boas-praticas/MVP_CD_Diabetes.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# MVP de Análise de Dados e Boas Práticas

## Profs. Tatiana Escovedo e Hugo Villamizar

### Curso: Pós Graduação em Análise de Dados e Analytics - PUC-RJ ([link](https://especializacao.ccec.puc-rio.br/especializacao/ciencia-de-dados-e-analytics))
### Aluno: Cláudio de Miranda Luz ([Linkedin](https://www.linkedin.com/in/claudiomluz/))

### Estrutura

De maneira geral o MVP contemplará as 3 etapas iniciais de um projeto de ciência de dados, sendo estas:

*   Definição do problema
*   Coleta e análise de dados
*   Pré-processamento

## 1. Definição do Problema

A plataforma **Airbnb** é um dos maiores exemplos de serviço de **economia compartilhada**. Proprietários ou administradores de imóveis podem oferecer acomodações de modo a obter retorno financeiro e hóspedes podem encontrar uma variedade de opções que atendam suas expectativas de experiência de hospedagem. O serviço possibilita o encontro dessas duas personas com cada um buscando seus melhores interesses.

No contexto de economia compartilhada, do ponto de vista da plataforma e do administrador do bem, uma **métrica** que pode ser um dos indicadores de melhor retorno de investimento seria a **taxa de ocupação**. No caso do Airbnb e de acomodações, quanto mais dias hóspedes estiverem usufruindo do bem, mais valor está sendo gerado para todas as partes envolvidas.

Minha proposta nesse trabalho é analisar dados da plataforma Airbnb referentes a acomodações na cidade do **Rio de Janeiro**. Esse estudo vai procurar observar e tratar as variáveis que podem estar mais diretamente ligadas à taxa de ocupação das acomodações assim como preparar os dados para que possam ser utilizados por algoritmos de machine learning mais pra frente.

O dataset usado neste projeto é proveniente do site [Inside Airbnb](http://insideairbnb.com/) e contém dados extraídos na data de 23 de setembro de 2023, a mais recentemente ofertada no momento da escrita do trabalho.

O **problema** que quero resolver depois da coleta, análise e do pré-processamento dos dados é encontrar os **fatores mais influentes para uma melhor taxa de ocupação** de uma acomodação e na sequência conseguir **estimar a taxa de ocupação** de uma acomodação **a partir de valores existentes** para esse fatores.

Esse problema se encaixa na categoria de **aprendizado supervisionado** num cenário de **regressão**, pois pretendo inferir valores de uma variável alvo a partir de valores de outras variáveis do dataset.

O dataset não contém diretamente um dado de taxa de ocupação então usarei outra variável, ligada a quantidade de avaliações da acomodação, como **variável proxy** para fazer a leitura da taxa de ocupação.

Tenho diversas **hipóteses** relacionadas ao tema, na relação de influência entre os dados e a variável alvo. Seguem algumas:

*   o bairro onde se localiza uma acomodação influencia na sua taxa de ocupação;
*   a proximidade com pontos de interesse influencia na taxa de ocupação;
*   existe sazonalidade para taxa de ocupação mesmo para acomodações muito demandadas;
*   imóveis menores tem taxa de ocupação mais alta do que os maiores;
*   o comportamento do anfitrião influencia na taxa de ocupação.

Ao longo do notebook farei a análise destas e de outras hipóteses sobre o tema.

Vou me referenciar aos imóveis ou quartos sempre chamando-os de **acomodação** e chamarei de **anfitrião** a pessoa que publicou a acomodação na plataforma Airbnb, seja ela proprietária ou não da acomodação.

O **dicionário de dados** sobre o dataset estará escrito no meio do notebook, após as etapas de limpeza iniciais, onde dados menos relevantes serão descartados. Serão enumerados e descritos no dicionário os dados originais mantidos e aqueles novos, gerados por enriquecimento, e que serão analisados mais profundamente nas sessões subsequentes.

Escolhi esse dataset por ser uma ótima opção para exercitar os conceitos aprendidos na disciplina. Ele contém dados numéricos, textuais, categóricos, datas e booleanos, além de ter dimensões interessantes. Buscarei aplicar as técnicas de pré-processamento aprendidas no curso, de maneira a fixar o conhecimento, e explorarei outras alternativas de tratamento e visualização.

## 2. Configurações iniciais

Criei essa seção para agregar configurações iniciais do notebook.

### 2.1 Imports
"""

# Importação de pacotes usados no notebook
import requests  # coleta de arquivos
import gzip  # descompactação de arquivos
import pandas as pd  # para análise de dados
import numpy as np
import missingno as ms  # para tratamento de missings
import warnings  # Pacote usado para configuração de warnings
from typing import List, Dict  # para usar em annotations de PEP 484
import json  # para interpretar listas que estão como string
import matplotlib.pyplot as plt  # pacotes de visualização
import seaborn as sns
from sklearn.model_selection import train_test_split  # pacotes para holdout
from sklearn.preprocessing import MinMaxScaler  # normalização
from sklearn.preprocessing import StandardScaler  # padronização
from sklearn.preprocessing import OrdinalEncoder  # ordinal encoding
from sklearn.preprocessing import OneHotEncoder  # one-hot e dummy encoding

# configuração para não exibir os warnings
warnings.filterwarnings("ignore")

"""### 2.2 Constantes"""

# Constantes que armazenanam os nomes de cada um dos atributos do dataset

# nomes originais das variáveis presentes no dataset
ID = 'id'
LISTING_URL = 'listing_url'
SCRAPE_ID = 'scrape_id'
LAST_SCRAPED = 'last_scraped'
SOURCE = 'source'
NAME = 'name'
DESCRIPTION = 'description'
NEIGHBORHOOD_OVERVIEW = 'neighborhood_overview'
PICTURE_URL = 'picture_url'
HOST_ID = 'host_id'
HOST_URL = 'host_url'
HOST_NAME = 'host_name'
HOST_SINCE = 'host_since'
HOST_LOCATION = 'host_location'
HOST_ABOUT = 'host_about'
HOST_RESPONSE_TIME = 'host_response_time'
HOST_RESPONSE_RATE = 'host_response_rate'
HOST_ACCEPTANCE_RATE = 'host_acceptance_rate'
HOST_IS_SUPERHOST = 'host_is_superhost'
HOST_THUMBNAIL_URL = 'host_thumbnail_url'
HOST_PICTURE_URL = 'host_picture_url'
HOST_NEIGHBOURHOOD = 'host_neighbourhood'
HOST_LISTINGS_COUNT = 'host_listings_count'
HOST_TOTAL_LISTINGS_COUNT = 'host_total_listings_count'
HOST_VERIFICATIONS = 'host_verifications'
HOST_HAS_PROFILE_PIC = 'host_has_profile_pic'
HOST_IDENTITY_VERIFIED = 'host_identity_verified'
NEIGHBOURHOOD = 'neighbourhood'
NEIGHBOURHOOD_CLEANSED = 'neighbourhood_cleansed'
NEIGHBOURHOOD_GROUP_CLEANSED = 'neighbourhood_group_cleansed'
LATITUDE = 'latitude'
LONGITUDE = 'longitude'
PROPERTY_TYPE = 'property_type'
ROOM_TYPE = 'room_type'
ACCOMMODATES = 'accommodates'
BATHROOMS = 'bathrooms'
BATHROOMS_TEXT = 'bathrooms_text'
BEDROOMS = 'bedrooms'
BEDS = 'beds'
AMENITIES = 'amenities'
PRICE = 'price'
MINIMUM_NIGHTS = 'minimum_nights'
MAXIMUM_NIGHTS = 'maximum_nights'
MINIMUM_MINIMUM_NIGHTS = 'minimum_minimum_nights'
MAXIMUM_MINIMUM_NIGHTS = 'maximum_minimum_nights'
MINIMUM_MAXIMUM_NIGHTS = 'minimum_maximum_nights'
MAXIMUM_MAXIMUM_NIGHTS = 'maximum_maximum_nights'
MINIMUM_NIGHTS_AVG_NTM = 'minimum_nights_avg_ntm'
MAXIMUM_NIGHTS_AVG_NTM = 'maximum_nights_avg_ntm'
CALENDAR_UPDATED = 'calendar_updated'
HAS_AVAILABILITY = 'has_availability'
AVAILABILITY_30 = 'availability_30'
AVAILABILITY_60 = 'availability_60'
AVAILABILITY_90 = 'availability_90'
AVAILABILITY_365 = 'availability_365'
CALENDAR_LAST_SCRAPED = 'calendar_last_scraped'
NUMBER_OF_REVIEWS = 'number_of_reviews'
NUMBER_OF_REVIEWS_LTM = 'number_of_reviews_ltm'
NUMBER_OF_REVIEWS_L30D = 'number_of_reviews_l30d'
FIRST_REVIEW = 'first_review'
LAST_REVIEW = 'last_review'
REVIEW_SCORES_RATING = 'review_scores_rating'
REVIEW_SCORES_ACCURACY = 'review_scores_accuracy'
REVIEW_SCORES_CLEANLINESS = 'review_scores_cleanliness'
REVIEW_SCORES_CHECKIN = 'review_scores_checkin'
REVIEW_SCORES_COMMUNICATION = 'review_scores_communication'
REVIEW_SCORES_LOCATION = 'review_scores_location'
REVIEW_SCORES_VALUE = 'review_scores_value'
LICENSE = 'license'
INSTANT_BOOKABLE = 'instant_bookable'
CALCULATED_HOST_LISTINGS_COUNT = 'calculated_host_listings_count'
CALCULATED_HOST_LISTINGS_COUNT_ENTIRE_HOMES = ('calculated_host_listings_'
                                               'count_entire_homes')
CALCULATED_HOST_LISTINGS_COUNT_PRIVATE_ROOMS = ('calculated_host_listings_'
                                                'count_private_rooms')
CALCULATED_HOST_LISTINGS_COUNT_SHARED_ROOMS = ('calculated_host_listings_'
                                               'count_shared_rooms')
REVIEWS_PER_MONTH = 'reviews_per_month'

# nomes de variáveis novas criadas no dataset
HOST_LOCATION_ADJUSTED = 'host_location_adjs'
HOST_PROXIMITY = 'host_proximity'
HAS_HOST_ABOUT = 'has_host_about'
BEDROOMS_ADPT = 'bedrooms_adpt'
BEDS_ADPT = 'beds_adpt'
AMENITIES_COUNT = 'amenities_count'
MONTHS_SINCE_REGISTERED = 'months_since_registered'
HAS_NEIGHBOURHOOD_OVERVIEW = 'has_neighbourhood_overview'
BATHS_INDEX = 'baths_index'
BATHS_CATEGORY = 'baths_category'

# descrições da cidade do Rio de Janeiro
RIO_DE_JANEIRO_DESCRIPTION = 'Rio de Janeiro, Brazil'
RIO_DE_JANEIRO_DESCRIPTION_ALT = 'Rio, Brazil'

# categorias de tipo de quarto
ROOMTYPE_ENTIRE = 'Entire home/apt'
ROOMTYPE_PRIVATE = 'Private room'
ROOMTYPE_SHARED = 'Shared room'
ROOMTYPE_HOTEL = 'Hotel room'

# categorias de banheiros
BATHROOM_DEDICATED = 'dedicated'
BATHROOM_SHARED = 'shared'
BATHROOM_NOT_PRESENT = 'no_bathroom'

# categorias de proximadade do host
HOST_PROXIMITY_VERYCLOSE = 'Very Close'
HOST_PROXIMITY_RELATIVELYCLOSE = 'Relatively Close'
HOST_PROXIMITY_FAR = 'far'
HOST_PROXIMITY_NOINFO = 'No Information'

"""### 2.3 Funções

#### 2.3.1 bathroom
"""

def get_bathrooms_index(row : pd.core.series.Series) -> float:
    '''
      Esse método visa transformar valores do campo bathrooms_text em um valor
      númerico com regras específicas
    '''
    texto_banheiros = row[BATHROOMS_TEXT]

    if pd.isna(texto_banheiros):
        return 0.0

    hospedes = row[ACCOMMODATES]

    lista = texto_banheiros.split()
    banheiros = 0.0

    try:
        # para a maioria dos casos o primeiro elemento da lista será um numeral
        banheiros = float(lista[0])
    except ValueError:
        # todos os casos em que o primeiro elemento da lista não é um número se
        # enquadram no cenário de half-bath (Half-bath, Shared half-bath e
        # Private half-bath)
        # vou assumir valor 0.5 para esses cenários
        banheiros = 0.5

    if banheiros == 0.0:
        return 0.0

    if 'shared' in texto_banheiros.lower():
        hospedes += 1

    return banheiros/hospedes

def get_bathrooms_category(row : pd.core.series.Series) -> str:
    '''
    Esse método visa interpretar o dado de banheiro e informar uma categoria
    entre dedicated, shared, ou no_bathroom
    '''
    texto_banheiros = row[BATHROOMS_TEXT]

    if pd.isna(texto_banheiros):
        return BATHROOM_NOT_PRESENT

    if 'shared' in texto_banheiros.lower():
        return BATHROOM_SHARED

    return BATHROOM_DEDICATED

"""#### 2.3.2 bedroom"""

def get_bedrooms_adjusted_value(row) -> float:
    '''
    Esse método visa transformar valores do campo bedrooms em um valor
    númerico com regras específicas
    '''
    # quantidade de quartos do objeto
    bedrooms_value = row[BEDROOMS]

    # verificando se tem valor nulo para aplicar regras
    if pd.isna(bedrooms_value):
        roomtype_value = row[ROOM_TYPE]

        if roomtype_value == ROOMTYPE_SHARED:
            # se for quarto compartilhado retorno a fração
            return 1 / row[ACCOMMODATES]
        else:
            return 1
    else:
        # se não for nulo retorno o próprio valor
        return bedrooms_value

"""#### 2.3.3 beds"""

def get_beds_adjusted_value_by_roomtype(row : pd.core.series.Series ,\
                                        medias : Dict) -> float:
    '''
    Esse método visa preencher valores para o campo beds que estejam nulos
    usando a média de acordo com o tipo de acomodação
    '''
    # quantidade de camas do objeto
    beds_value = row[BEDS]

    # verificando se tem valor nulo para aplicar regras
    if pd.isna(beds_value):
        # retornando a média de camas de acordo com o tipo de acomodacao
        return medias.get(row[ROOM_TYPE])
    else:
        # se não for nulo retorno o próprio valor
        return beds_value

"""#### 2.3.4 amenities"""

def get_amenities_count(row : pd.core.series.Series) -> float:
    '''
    Esse método visa converter a lista de comodidades num número que indique
    a quantidade de itens
    '''
    # lista de comodidades
    amenities = row[AMENITIES]

    # verificando se tem valor nulo para aplicar regras
    if pd.isna(amenities):
        # retornando 0 se for nulo
        return 0.0
    else:
        # se não for nulo retorno a quantidade de itens da lista
        return len(json.loads(amenities))

"""#### 2.3.5 reviews"""

def calcula_meses_desde_registro(row : pd.core.series.Series) -> int:
    '''
    Calcular a quantidade de meses desde o registro de uma acomodação a partir
    de valores de number_of_reviews e reviews_per_month
    '''
    resultado_float = row[NUMBER_OF_REVIEWS]/row[REVIEWS_PER_MONTH]
    return round(resultado_float)

"""#### 2.3.6 host_location"""

def fill_in_rio(row : pd.core.series.Series, bairros_referencia : List) -> str:
  '''
  Essa função visa preencher o valor que representa a cidade do Rio de Janeiro
  nos casos pertinentes
  '''
  if row[HOST_LOCATION] is np.NaN:
      # lista de bairros foi carregada na variável bairros_referencia na seção
      # da Entidade Localização da Acomodação
      if row[HOST_NEIGHBOURHOOD] in bairros_referencia:
          return RIO_DE_JANEIRO_DESCRIPTION
  return row[HOST_LOCATION]

def host_proximity(row : pd.core.series.Series) -> str:
    '''
    Essa função visa criar uma nova variável com categorias que indiquem
    proximidade do host com a acomodação administrada
    '''
    rio_city_descriptions = [RIO_DE_JANEIRO_DESCRIPTION,
                             RIO_DE_JANEIRO_DESCRIPTION_ALT]
    cidade_anfitriao = row[HOST_LOCATION_ADJUSTED]
    bairro = row[HOST_NEIGHBOURHOOD]

    if cidade_anfitriao is np.NaN:
        return HOST_PROXIMITY_NOINFO
    else:
        if cidade_anfitriao in rio_city_descriptions:
            # pode ser cidade e bairro ou apenas cidade
            # lista de bairros foi carregada na variável bairros_referencia na
            # seção da Entidade Localização da Acomodação
            if bairro in bairros_referencia:
                return HOST_PROXIMITY_VERYCLOSE
            else:
                return HOST_PROXIMITY_RELATIVELYCLOSE
        else:
            return HOST_PROXIMITY_FAR

"""#### 2.3.7 Visualização de dados"""

def render_histogram_chart(data : pd.core.frame.DataFrame, bins : int) -> None:
    '''
    Renderiza um gráfico do tipo histograma aceitando definição de quantidade
    de bins
    '''
    plt.title('Histogram')
    plt.hist(data, bins=bins, color='skyblue', edgecolor='black')
    plt.xlabel(data.name)
    plt.show()

def render_one_boxplot(data : pd.core.frame.DataFrame, outliers : bool) -> None:
    '''
    renderizar um gráfico do tipo boxplot e incluir 1o e 3o quartis, além
    da mediana
    '''
    # Criar o boxplot
    plt.boxplot(data, showfliers=outliers)

    # Obter estatísticas do boxplot
    median = np.median(data)
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)

    # Adicionar textos ao gráfico
    plt.text(1.1, median, f'Median: {median:.2f}', verticalalignment='center',
             color='red', fontweight='bold')
    plt.text(1.1, q1, f'Q1: {q1:.2f}', verticalalignment='bottom',
             color='blue', fontweight='bold')
    plt.text(1.1, q3, f'Q3: {q3:.2f}', verticalalignment='top',
             color='green', fontweight='bold')

    # adicionado o nome da variável
    plt.xlabel(data.name)

    # Exibir o gráfico
    plt.show()

def render_density_chart(data : pd.core.frame.DataFrame) -> None:
    '''
    Renderizar um gráfico de densidade
    '''
    # Criar um gráfico de densidade para uma única variável
    sns.kdeplot(data=data, fill=True)

    # Adicionar rótulos e título
    plt.xlabel(data.name)
    plt.ylabel('Density')
    plt.title('Density Curve')

    # Exibir o gráfico
    plt.show()

def render_bar_and_donut_charts(data : pd.core.frame.DataFrame) -> None:
    '''
    renderiza um gráfico de barras e um donut lado a lado
    '''
    # Criar a figura e os subplots
    fig, axs = plt.subplots(1, 2, figsize=(10, 4))

    # Contar as ocorrências de cada categoria
    contagem_categorias = data.value_counts()

    # Gráfico de barras
    axs[0].bar(contagem_categorias.index, contagem_categorias.values,
               color='skyblue')
    axs[0].set_title(f'{data.name} bar chart')
    # Quebra de linha nos rótulos do eixo x
    axs[0].set_xticklabels(contagem_categorias.index, rotation=0, ha='center',
                           fontsize=8)  # Configura rótulos com quebra de linha

    if pd.api.types.is_bool_dtype(data):
      axs[0].set_xticks([True, False])
      axs[0].set_xticklabels(['True', 'False'])

    # Gráfico donut
    # não exibo o percentual se for menor do que 1%
    axs[1].pie(contagem_categorias.values, labels=contagem_categorias.index,
               autopct=lambda p: f'{p:.1f}%' if p >= 1 else '',
               startangle=90, wedgeprops=dict(width=0.2, edgecolor='w'))
    axs[1].set_title(f'{data.name} donut chart')

    # Ajustar o layout para evitar sobreposição
    plt.tight_layout()

    # Exibir os gráficos
    plt.show()

def render_bar_chart_horizontal(data : pd.core.frame.DataFrame) -> None:
    '''
    Renderiza um gráfico de barras na horizontal
    '''
    # Contar a quantidade de cada categoria
    contagem_por_categoria = data.value_counts()

    # Ordenar as categorias por contagem em ordem decrescente
    contagem_por_categoria = contagem_por_categoria.sort_values(ascending=True)

    # Aumentar o tamanho do gráfico verticalmente
    plt.figure(figsize=(8, 24))

    # Criar o gráfico de barras horizontais
    plt.barh(contagem_por_categoria.index, contagem_por_categoria,
            color='lightcoral')

    # Adicionar rótulos e título
    plt.xlabel('Amount')
    plt.ylabel(data.name)

    # Exibir o gráfico
    plt.show()

def render_value_counts_as_line(data : pd.core.frame.DataFrame) -> None:
    '''
    Renderiza a métrica retornada por value_counts num gráfico de linha
    '''
    # Usar value_counts para obter a contagem de cada valor
    contagens = data.value_counts()

    # Ordenar os valores para garantir a ordem correta no gráfico
    contagens = contagens.sort_index()

    # Criar o gráfico de linhas
    plt.plot(contagens.index, contagens.values, linestyle='-')

    # Adicionar rótulos e título
    plt.xlabel('Values')
    plt.ylabel('Count')
    plt.title(data.name)

    # Exibir o gráfico
    plt.show()

def render_value_counts_as_dots(data : pd.core.frame.DataFrame) -> None:
    '''
    Renderiza a métrica retornada por value_counts num gráfico de pontos
    '''
    # Usar value_counts para obter a contagem de cada valor
    contagens = data.value_counts()

    # Ordenar os valores para garantir a ordem correta no gráfico
    contagens = contagens.sort_index()

    # Criar um gráfico de dispersão (scatter) sem linhas
    plt.scatter(contagens.index, contagens.values, marker='.', color='blue')

    # Adicionar rótulos e título
    plt.xlabel('Instances')
    plt.ylabel('Amount')
    plt.title(f'Scatter plot for {data.name}')

    # Exibir o gráfico
    plt.show()

def render_box_plot_as_small_multiples(data : pd.core.frame.DataFrame,
                                       num_var : str , category_var : str,
                                       outliers : bool) -> None:
    '''
    Renderiza vários boxplots como pequenos múltiplos
    '''
    # Define o tamanho da figura
    plt.figure(figsize=(8, 6))

    # Cria um gráfico de boxplot com pequenos múltiplos usando Seaborn
    sns.boxplot(x=category_var, y=num_var, data=data, showfliers=outliers)

    # Adiciona títulos e rótulos aos eixos
    plt.title(f"{num_var} by {category_var} boxplots")
    plt.xlabel(category_var)
    plt.ylabel(num_var)

    # Exibe o gráfico
    plt.show()

def render_multiple_horizontal_box_plots(data : pd.core.frame.DataFrame,
                                         category_column : str,
                                         numeric_column : str,
                                         minimum_itens: int = -1) -> None:
    '''
    Renderiza vários boxplots de maneira horizontal
    '''

    if minimum_itens != -1:
        data = filter_to_top_category_itens(data, category_column,
                                            minimum_itens)

    # Calcula as medianas de cada categorias
    medianas = data.groupby(category_column)[numeric_column].median()
    medianas_sem_na = medianas.dropna()

    # Ordena as categorias com base nas medianas
    categorias_ordenadas = medianas_sem_na.sort_values(ascending=False).index

    # obtenho o total de itens
    tam = len(categorias_ordenadas)

    # Define o tamanho da figura
    plt.figure(figsize=(12, tam/2))

    # Cria um boxplot horizontal usando Seaborn
    sns.boxplot(x=numeric_column, y=category_column, data=data,
                order=categorias_ordenadas, orient='h')

    # Adiciona títulos e rótulos aos eixos
    plt.title("Múltiplos Box Plots na Horizontal (Ordenados pela Mediana)")
    plt.xlabel("Valores Numéricos")
    plt.ylabel("Categorias")

    # Exibe o gráfico
    plt.show()

def get_color_pallete_as_csat() -> sns.palettes._ColorPalette:
    '''
    Essa função retorna uma paleta de cores que simula uma remete ao
    padrão csat, de satisfação do consumidor.
    '''
    # definindo intervalos a cada 0.25
    intervalos = np.arange(0, 5.25, 0.25)

    # gerei um gradiente por conta própria usando esse site:
    # https://colordesigner.io/gradient-generator
    gradiente_vermelho = ['#8b0000','#9a1c02','#a82f01','#b64000','#c35100',
                          '#d06100','#dd7200','#e98300','#f49400','#ffa500']
    gradiente_amarelo = ['#ffbb00','#ffd200','#fde800','#f6ff08']
    gradiente_verde_claro = ['#daff00', '#bbff00', '#98ff00', '#6bff08']
    gradiente_verde_escuro = ['#4dbc02', '#317e00']

    # montando uma lista com todas as cores acima
    cores = gradiente_vermelho + gradiente_amarelo + \
            gradiente_verde_claro + gradiente_verde_escuro

    # Criando a paleta de cores usando color_palette
    paleta_cores = sns.color_palette(cores)
    return paleta_cores

"""#### 2.3.8 Outras estatísticas"""

def print_cumulative_percentage(data : pd.core.frame.DataFrame):
  '''
  Imprimir tabela contendo dado acumulativo da contagem
  '''
  # Contar a quantidade de cada categoria
  contagem_por_categoria = data.value_counts()

  # Ordenar as categorias por contagem em ordem decrescente
  contagem_por_categoria = contagem_por_categoria.sort_values(ascending=False)

  # Calcular percentual acumulado
  percentual_acumulado = contagem_por_categoria.cumsum() /\
                         contagem_por_categoria.sum() * 100

  # Criar a tabela
  tabela = pd.DataFrame({
      'Category': contagem_por_categoria.index,
      'Amount': contagem_por_categoria.values,
      'Cumulative percentage': percentual_acumulado.round(1).values
  })

  # Imprimir a tabela
  print(tabela)

"""#### 2.3.9 Utils"""

def filter_to_top_category_itens(data : pd.core.frame.DataFrame,
                                 category_column : str,
                                 count_greater_than : int) \
                                 -> pd.core.frame.DataFrame:
    '''
    Esse função analisa um dataset data fazendo uma contagem de ocorrências
    de cada categoria da coluna category_column e retona as categorias que
    tiveram contagem acima do valor passado como greater_than
    '''
    # Use value_counts para obter as contagens
    contagens_categoria = data[category_column].value_counts()

    # Selecione as categorias com contagens acima de limit
    categorias_acima_de_limit =\
        contagens_categoria[contagens_categoria.values > count_greater_than]

    # Filtrar o DataFrame para incluir apenas as categorias acima de limit
    df_filtrado = data[data[category_column].\
                       isin(categorias_acima_de_limit.index)]

    return df_filtrado

"""## 3. Coleta e análise dados

### 3.1 Coleta dos dados

Carregarei o arquivo a partir de uma URL, usarei uma biblioteca para fazer a descompressão do arquivo e criarei um dataset usando o pacote Pandas (Python Data Analysis Library). O arquivo já vem com cabeçalho.
"""

# Carrega arquivo csv proveniente de uma URL usando Pandas
# Informa a URL de importação do dataset

host = 'http://data.insideairbnb.com'
path_country_city = '/brazil/rj/rio-de-janeiro'
path_file = '/2023-09-22/data/listings.csv.gz'
url_listings = host + path_country_city + path_file

print(url_listings)

# Nome do arquivo após o download
nome_arquivo_local_listings = 'listings.csv.gz'

# Faz o download do arquivo
response_listings = requests.get(url_listings)

# Verifica se o download foi bem-sucedido
if response_listings.status_code == 200:
    # Salva o conteúdo em um arquivo local
    with open(nome_arquivo_local_listings, 'wb') as file:
        file.write(response_listings.content)

    # Abre o arquivo compactado
    with gzip.open(nome_arquivo_local_listings, 'rt') as file:
        ds = pd.read_csv(file)

    # Agora, 'ds' contém seus dados do dataset
else:
    print("Erro ao fazer o download do arquivo.")

"""### 3.2 Análise dos dados

Nessa seção, farei uma análise inicial do dataset.

#### 3.2.1 Análise de metadados

Primeiramente farei uma análise de metadados do dataset observando não o seu conteúdo mas sim suas dimensões e variáveis envolvidas.
"""

# Análise de dimensões do dataset
print('Quantidade de linhas e quantidade de colunas:' , ds.shape)

# Análise dos nomes da variáveis e seus tipos de dados
# Configurar para exibir todas as colunas
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# Exibir os tipos de dados de todas as variáveis
print(ds.dtypes)

"""A análise feita até agora já indica que algumas ações de limpeza de dados podem ser executadas. O dataset tem 75 variáveis e parece haver oportunidade para redução da dimensionalidade.

A análise dos nomes das variáveis, sem mesmo uma leitura do dicionário de dados, já sinaliza a possibilidade de ações de redução da quantidade de variáveis:

*   dados redundantes ou duplicados, como é o caso de bathrooms e bathrooms_text;
*   dados de localização redundantes como pode ser o caso de neighbourhood,
neighbourhood_cleansed, neighbourhood_group_cleansed, latitude e longitude.
*   dados sensíveis quanto à privacidade, como é o caso de host_name;
*   metadados não relevantes para a análise como scrape_id e last_scraped;
*   dados possivelmente excessivamente detalhados como seria o caso de review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location e review_scores_value.

Por enquanto os itens mencionados acima são apenas hipóteses para redução da dimensionalidade e uma análise mais detalhada é essencial para a correta tomada de decisão.

Outro observação importante se refere ao tipo do dado. A variável price, por exemplo, está sendo tratada como object e é razoável considerar que ela deva ser tratada como valor numérico.

O [dicionário de dados](https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=1322284596) fornecido pelo próprio site **Inside Airbnb** contribui para uma análise qualitativa sobre cada variável de modo que a decisão sobre eliminação de algumas delas possa ser tomada. Mais adiante, depois da redução de dimensionalidade, escreverei um dicionário de dados como parte desse trabalho, para melhor entendimento do dataset.

A observação mais importante a ser colocada aqui, que tem relação direta com o problema a ser resolvido no trabalho, se refere à ausência de uma variável que indique a taxa de ocupação das acomodações. Fará parte desse trabalho a análise mais detalhada do conjunto de variáveis para escolha de uma variável proxy que possa ser usada como indicadora indireta da taxa de ocupação. Esse tema será tratado na seção de pré-processamento dos dados.

"""

# Analisando a completude de dados de cada variável
ds.count()

"""O resultado da contagem de dados de cada variável, apresentado na seção anterior, já indica que algumas delas não contém dados para todos os 31964 itens do dataset.

*   4 variáveis não contém dados e podem ser eliminadas;
*   as variáveis de review contém dados para pouco menos de 24 mil itens do dataset o que indica a possibilidade de eliminação de parte do conjunto;
*   algumas variáveis como neighborhood_overview e host_about estão parcialmente preenchidas e podem ser consideradas para uma possível eliminação completa da variável, para uma limpeza parcial dos itens que não tem esse valor preenchido, ou mesmo para estratégias de enriquecimento. Essa decisão será tomada no momento de análise mais específica do conteúdo.

#### 3.2.2 Análise dos dados (conteúdo)

Agora faço uma análise simples do conteúdo do dataset verificando valores das linhas iniciais e finais do dataset.
"""

# exibição das linhas iniciais do dataset
ds.head()

# exibição das linhas finais do dataset
ds.tail()

"""Após analisar essa pequena amostra sobre os dados, já é possível confirmamos algumas das hipóteses consideradas para limpeza de dados mencionadas anteriormente e considerarmos outras:

*   Variáveis contendo textos longos como name e description poderiam ser tratadas com diferentes tipos de abordagem. Uma opção seria tratá-las como booleanos indicando se foram preenchidas ou não. Outra abordagem seria usar técnica de NLP para processamento de linguagem natural, avaliando de fato o conteúdo presente. Vou optar por ignorar essas variáveis na análise assumindo que elas sempre estarão preenchidas com dados relevantes.
*   Nomes de hosts estão realmente expostos o que implica em questões de privacidade de dados;
*   A variável host_response_time parece ser uma variável categórica e poderia ser convertida numa variável numérica indicando qualidade, com menor sendo melhor;
*   A variável neighbourhood, nessa pequena amostra, contém o nome da cidade ou não contém dados, o que indica que podemos eliminá-la, pois já estamos limitados ao Rio de Janeiro no trabalho;
*   A variável bathrooms pode ser eliminada e a variável bathrooms_text pode ser tratada para ser convertida em valor numérico;
*   A variável amenities contém uma lista de itens e pode ser tratada para que os itens virem variáveis em separado no dataset caso uma avaliação indique que são itens relevantes para o problema proposto;
*   As variáveis property_type e room_type podem ter redundância e uma análise mais detalhada pode indicar que apenas uma delas prevaleça. Além disso, são variáveis categóricas que podem ser convertidas em valores binários;
*  As variáveis latitude e longitude são muito específicas na localização e podem ser eliminadas se quisermos considerar localidade apenas a nível de bairro.  

Essas são apenas algumas das observações possíveis de serem feitas da pequena amostra do dataset apresentada na seção anterior. Considerarei essas e outras mais na seção subsequente quando farei pré-processamento dos dados.

Na seção seguinte apresento alguns dados estatísticos sobre variáveis numéricas selecionadas, para geração de insights e identificação de cenários interessantes.
"""

# selecionando as variáveis que estarão presentes na análise estatística
columns_to_describe = ['host_listings_count','accommodates','bedrooms','beds','minimum_nights','maximum_nights','number_of_reviews']

# obtendo a análise de cada variável
description = ds[columns_to_describe].describe()

# imprimindo a análise
print(description)

"""Foram selecionadas 7 variáveis numéricas e algumas observações interessantes podem ser identificadas:

*   **host_listings_count** : Na média um gestor de acomodações controla 16 itens mas notem que a mediana indica apenas 2. O valor de max indica que um host controla 1311 acomodações. Isso explica o valor bem alto de desvio padrão e sinaliza uma alta dispersão.
*   **accommodates** : Na média as acomodações aceitam 4 pessoas e a mediana também indica isso. Existe uma acomodação que aceita 16 pessoas. Eu esperava valores maiores como máximo nessa variável. É possível que esse máximo seja definido pelo próprio Airbnb.
*   **bedrooms** : Achei que esse dado tivesse um desvio padrão maior, com mais acomodações indicando mais quartos. Veremos em mais detalhe na seção de visualizações de dados mais para frente. Existe uma propriedade com 26 quartos.
*   **beds** : Quantidade de camas parece ter uma relação direta com quartos e existe uma propriedade com 91 camas. A média está bem próxima da mediana.
*   **minimum_nights** : A quantidade mínima de noites para as reservas indica uma média de 4 noites mas notem a mediana indicando 2 noites. Algumas propriedades permitem reserva mínima bem grande e por isso acontece a diferença entre média e mediana. Uma propriedade tem valor mínimo de noites com valor 1125 o que seria aproximadamente 3 anos, ou seja, o hóspede só faz a reserva se aceitar fazê-la por 3 anos. Vou considerar eliminar esses itens para o trabalho.
*   **maximum_nights** : A quantidade máxima de noites também apresenta alguns valores estatísticos interessantes. A mediana indica 365 dias, ou seja, 1 ano. O p75 indica 1125 noites, ou seja, 3 anos. E existe alguém que aceita como máximo uma reserva de 1825 dias, ou seja, 5 anos.
*   **number_of_reviews** : A mediana indica 4 reviews por acomodação com P75 com valor 20. Existe uma acomodação com 618 reviews. O desvio padrão para essa variável está praticamente o dobro da média..

#### 3.2.3 Playground

Criei essa seção e chamei-a de playground por ser uma parte mais divertida, onde procurarei analisar em mais detalhes os cenários extremos encontrados na análise estatística feita previamente. Esse tipo de abordagem favorece a geração de insights.

O primeiro outlier identificado é o anfitrião que mais tem acomodações sob sua gestão.
"""

# identificação do anfitrião gestor de milhares de acomodações
acomodacao_um_do_super_host = ds[ds['host_listings_count'] == 1311].head(1)

host_name = acomodacao_um_do_super_host['host_name']
host_location = acomodacao_um_do_super_host['host_location']
host_about = acomodacao_um_do_super_host['host_about']
host_url = acomodacao_um_do_super_host['host_url']

print ("O nome do super host é \033[1m%s\033[0m e ele está localizado em \033[1m%s\033[0m.\n" % (host_name.values[0],host_location.values[0]))
print ("Esse host se apresenta da seguinte maneira:\n\n\033[1m%s\033[0m\n\n" % host_about.values[0])
print ("A página inicial desse host na plataforma Airbnb é: %s" % host_url.values[0])

"""Agora uma análise sobre as acomodações que aceitam 16 hóspedes, valor indicado como máximo na análise estatística."""

# Contagem das acomodações que aceitam 16 hóspedes.
print ("Existem %s acomodações que aceitam 16 hóspedes" % ds[ds['accommodates'] == 16].shape[0])

"""De fato a plataforma Airbnb define 16 como a quantidade máxima de hóspedes em uma acomodação. Essa informação pode ser confirmada nesse [link](https://news.airbnb.com/br/airbnb-limita-a-16-numero-de-pessoas-por-estadia/#:~:text=O%20Airbnb%20anuncia%20hoje%20que,respons%C3%A1veis%20no%20contexto%20da%20pandemia.) da plataforma.


Agora a identificação da(s) propriedade(s) com 26 quartos.
"""

# identificação da acomodação com 26 quartos

acomodacao_26_quartos = ds[ds['bedrooms'] == 26]

listing_url = acomodacao_26_quartos['listing_url'].values[0]
neighbourhood_cleansed = acomodacao_26_quartos['neighbourhood_cleansed'].values[0]

print ("A acomodação que contém 26 quartos está localizada no bairro \033[1m%s\033[0m e pode ser consultada através desse link: %s\n" % (neighbourhood_cleansed,listing_url))

"""Identifico agora a propriedade com 91 camas."""

# identificação da acomodação com 91 camas

acomodacao_91_camas = ds[ds['beds'] == 91]

listing_url = acomodacao_91_camas['listing_url'].values[0]
neighbourhood_cleansed = acomodacao_91_camas['neighbourhood_cleansed'].values[0]

print ("A acomodação que contém 91 camas está localizada no bairro \033[1m%s\033[0m e pode ser consultada através desse link: %s\n" % (neighbourhood_cleansed,listing_url))

"""Identificando agora as acomodações outliers para as variáveis de número mínimo de noites e numero máximo de noites."""

# identificação das acomodações outliers para número mínimo de noite (1125 noites) e numéro máximo de noites (1825 noites)

acomodacao_1125_noites = ds[ds['minimum_nights'] == 1125]
acomodacao_1825_noites = ds[ds['maximum_nights'] == 1825]

listing_url_min = acomodacao_1125_noites['listing_url'].values[0]
listing_url_max = acomodacao_1825_noites['listing_url'].values[0]
neighbourhood_cleansed_min = acomodacao_1125_noites['neighbourhood_cleansed'].values[0]
neighbourhood_cleansed_max = acomodacao_1825_noites['neighbourhood_cleansed'].values[0]

print ("A acomodação que limita reservas para um mínimo de 1125 dias (3 anos) está localizada no bairro \033[1m%s\033[0m e pode ser consultada através desse link: %s\n" % (neighbourhood_cleansed_min,listing_url_min))

print ("A acomodação que limita reservas para um máximo de 1825 dias (5 anos) está localizada no bairro \033[1m%s\033[0m e pode ser consultada através desse link: %s\n" % (neighbourhood_cleansed_max,listing_url_max))

"""Para finalizar essa seção, identifico a acomodação que tem o maior número de reviews: 618."""

# identificação da acomodação com 618 reviews

acomodacao_618_reviews = ds[ds['number_of_reviews'] == 618]

listing_url = acomodacao_618_reviews['listing_url'].values[0]
neighbourhood_cleansed = acomodacao_618_reviews['neighbourhood_cleansed'].values[0]

print ("A acomodação que tem 618 reviews está localizada no bairro \033[1m%s\033[0m e pode ser consultada através desse link: %s\n" % (neighbourhood_cleansed,listing_url))

"""Note que a quantidade de reviews pode estar diferente ao acessar a url visto que a acomodação já pode ter sido reservada mais vezes depois da data da extração dos dados.

#### 3.2.4 Visualizações de dados antes da limpeza

Incluí essa seção para termos uma visualização inicial do dataset antes de qualquer ação de limpeza.
"""

# exibindo visualização matricial da nulidade do dataset
ms.matrix(ds)

"""Notem que essa visualização confirma informações obtidas nas seções anteriores, sobre variáveis sem dados e variáveis com preenchimento parcial.

## 4 Pré-Processamento

Essa seção contemplará ações de limpeza dos dados, enriquecimento e preparação do dataset para etapas futuras.

Dados que não agregam valor para a resolução do problema proposto serão descartados e outros dados poderão ser adicionados, se contribuirem para uma melhor interpretação do dataset e para melhor tratarmos o problema. Farei uma abordagem inicial de *feature selection*.

Os detalhamentos sobre cada uma destas etapas aparecerão nas seções seguintes.

### 4.1 Definição da variável alvo

Incluí essa seção logo no início da etapa de pré-processamento para evitar o risco de descartar alguma variável que seja útil para o estabelecimento da variável alvo, ainda inexistente.

A variável alvo proposta na definição do problema é a taxa de ocupação da acomodação. Uma taxa de ocupação é calculada com uma relação que divide o uso pela disponibilidade. Se um bem está disponível por 10 minutos e é usado por 9, sua taxa de uso é de 90%. Como as reservas das acomodações acontecem com contagem de dias, é natural que pensemos em usar o dia como unidade de contagem.

Notem que calcular uma taxa de ocupação envolve olhar para o passado. Eu preciso analisar a disponibilidade no passado e a utilização no passado para fazer a conta.

O melhor dado presente no dataset que entrega informações de uso da acomodação no passado é a quantidade de reviews.

Existem 4 variáveis que posso considerar:

| Variáveis | Descrição |
|-----------|-----------|
|number_of_reviews | Número total de reviews da acomodação |
|number_of_reviews_ltm | Número total de reviews nos últimos 12 meses |
|number_of_reviews_l30d | Número total de reviews nos últimos 30 dias |
|reviews_per_month | Número de reviews por mês durante todo o período de existência da acomodação na plataforma |

Descarto number_of_reviews pois é uma variável que vai exigir uma análise usando o período de existência de cada acomodação o que pode envolver tanto períodos muito curtos como períodos muito longos, agregando muita influência de sazonalidade e outros fatores temporais não controlados no resultado. Também descarto reviews_per_month pelos mesmos motivos.

Dentre **number_of_reviews_ltm** e number_of_reviews_l30d, entendo que a primeira contempla um horizonte de tempo mais significativo e abrangente, de 12 meses, para o resultado esperado. Cobre as quatro estações do ano e períodos de alta e baixa temporada de turismo, além de ser um horizonte de tempo razoável para entender qualquer comportamento diferenciado nos dados, caso apareça.

No meu entendimento esse é o melhor dado que podemos usar para inferir a taxa de ocupação das acomodações mas não podemos nos esquecer de que escrever o review não é uma ação obrigatória para os hóspedes então a relação entre estadias e quantidades de reviews não é 1 para 1.

Analisarei em mais detalhe essa variável numa seção posterior ainda aqui na seção de pré-processamento.

### 4.2 Redução de dimensionalidade

Farei algumas ações de *feature selection* com o objetivo não só de simplificar esse trabalho mas também de buscar eficiência computacional no tratamento dos dados.

Antes de iniciar as ações de redução de dimensionalidade, eu proponho uma análise qualitativa em relação ao dataset e uma organização da informação segundo uma visão de entidades relevantes.

Essa modelagem conceitual organiza os atributos segundo as entidades: **Acomodação**, **Localização da Acomodação**, **Anfitrião** e **Outros**.

<br>

Acomodação  | Localização da Acomodação | Anfitrião | Outros
-------------------|------------------|------------------|------------------
id                 |neighborhood_overview| host_id  |    scrape_id              
listing_url        | neighbourhood |  host_url    | last_scraped
name               | neighbourhood_cleansed |   host_name |       source      
description        |neighbourhood_group_cleansed| host_since |
picture_url             | latitude | host_location
property_type           | longitude |    host_about              
room_type               |     |   host_response_time                
accommodates            |     |    host_response_rate              
bathrooms               |     |   host_acceptance_rate           
bathrooms_text          |     |    host_is_superhost         
bedrooms                |     |   host_thumbnail_url       
beds                    |     |  host_picture_url      
amenities               |     |   host_neighbourhood    
price                   |     |    host_listings_count
minimum_nights          |     |    host_total_listings_count
maximum_nights          |     |  host_verifications
minimum_minimum_nights  |     |   host_has_profile_pic        
maximum_minimum_nights  |     |     host_identity_verified    
minimum_maximum_nights  |     |   calculated_host_listings_count    
maximum_maximum_nights  |     |   calculated_host_listings_count_entire_homes  
minimum_nights_avg_ntm  |     | calculated_host_listings_count_private_rooms
maximum_nights_avg_ntm  |     | calculated_host_listings_count_shared_rooms
calendar_updated        |       
has_availability        |       
availability_30         |                         
availability_60         |                        
availability_90         |                        
availability_365        |                        
calendar_last_scraped   |                       
number_of_reviews       |                        
number_of_reviews_ltm   |                        
number_of_reviews_l30d  |                        
first_review            |                         
last_review             |                         
review_scores_rating    |                        
review_scores_accuracy  |                        
review_scores_cleanliness |                      
review_scores_checkin    |                       
review_scores_communication |                    
review_scores_location      |                   
review_scores_value         |                    
license                     |                    
instant_bookable            |                     
reviews_per_month |

Incluo também um trecho de código para inicializar um dataset em separado para sofrer as operações de limpeza, mantendo o dataset original com seus dados.
"""

# criação de um novo dataset para executar as operações de limpeza

# recuperando os nomes das colunas
col = list(ds.columns)

# o novo dataset irá conter todas as colunas do dataset original
ds_cleansed = ds[col[:]]

"""#### 4.2.1 Redução de dimensionalidade da entidade Acomodação

Removendo colunas que não contém dados.
"""

# removendo as colunas bathrooms, calendar_updated e license pois não contém dados
ds_cleansed.drop([BATHROOMS, CALENDAR_UPDATED, LICENSE], axis=1, inplace= True)

"""Removendo a coluna **id** por não contribuir para a análise e a estatística do dataset."""

# removendo id
ds_cleansed.drop([ID], axis=1, inplace= True)

"""Removendo coluna **name**, que se refere a um título dado para a acomodação, por que é um dado presente em todas as instâncias e a diferenciação da qualidade do preenchimento desse título exigiria uma esforço com uso de técnicas de processamento de linguagem natural que não são escopo desse trabalho."""

# removendo name
ds_cleansed.drop([NAME], axis=1, inplace= True)

"""Eu poderia usar duas abordagens para considerar a coluna **description** como influenciadora da variável alvo de taxa de ocupação.

1. Uma acomodação com description preenchida provavelmente teria uma taxa de
ocupação melhor do que uma com esse dado vazio. No entanto, quase a totalidade do dataset tem esse valor preenchido (31305 de 31964) então usar essa técnica não traria tanto benefício.
2. Outra possibilidade seria fazer uma avaliação qualitativa do texto escrito, com técnicas de processamento de linguagem natural, para transformar esse texto num dado categorizado e ordenado

Opto por remover a coluna description por questões de simplificação da análise e do trabalho.
"""

# removendo description
ds_cleansed.drop([DESCRIPTION], axis=1, inplace= True)

"""Removo a coluna **picture_url** e faço considerações similares aquelas feitas para description. Poderíamos usar técnicas mais avançadas de visão computacional para a avaliação da imagem mas não faz parte do escopo desse trabalho."""

# removendo picture_url
ds_cleansed.drop([PICTURE_URL], axis=1, inplace= True)

"""As variáveis relacionadas aos valores mínimo e máximo de dias para uma reserva de hospedagem na acomodação são: **minimum_nights, maximum_nights, minimum_minimum_nights, maximum_minimum_nights, minimum_maximum_nights, maximum_maximum_nights, minimum_nights_avg_ntm e maximum_nights_avg_ntm**. Enquanto as duas primeiras são valores padrão para a acomodação, as outras são valores configurados pelo anfitrião para datas futuras no calendário.

Por questões de simplificação, vou manter no dataset apenas os valores padrão da acomodação removendo as outras.

"""

# removendo minimum_minimum_nights, maximum_minimum_nights, minimum_maximum_nights, maximum_maximum_nights, minimum_nights_avg_ntm, maximum_nights_avg_ntm
ds_cleansed.drop([MINIMUM_MINIMUM_NIGHTS, MAXIMUM_MINIMUM_NIGHTS, MINIMUM_MAXIMUM_NIGHTS, MAXIMUM_MAXIMUM_NIGHTS, MINIMUM_NIGHTS_AVG_NTM, MAXIMUM_NIGHTS_AVG_NTM], axis=1, inplace= True)

"""Existem variáveis no dataset que informam sobre disponibilidade de reserva para a acomodação. São elas: **has_availability, availability_30, availability_60, availability_90 e availability_365**.

Enquanto a primeira indica apenas se há ou não disponibilidade independente do horizonte de tempo, as subsequentes indicam a quantidade de dias disponíveis no horizonte de tempo mencionado no próprio nome da variável.

A princípio imagino que todas as acomodações estariam disponíveis para reserva mas uma análise simples nesse dado mostra que isso não é verdade.

"""

# Verifica a distribuição para a variável has_availability
ds_cleansed.groupby('has_availability').size()

"""Podemos observar que existem acomodações que não tem disponibilidade de reserva. Isso poderia acontecer por serem casos de super sucesso onde hóspedes já pegaram todas as datas disponíveis ou simplesmente pelo fato do anfitrião não querer disponibilizar a acomodação naquele momento. Eu apostaria bem mais no segundo cenário.

Como a informação do porquê da indisponibilidade não está presente e as possíveis motivações para ela são completamente antagônicas para influência de taxa de ocupação, eu opto por remover essas instâncias do dataset.
"""

# removendo instâncias
ds_cleansed = ds_cleansed.query("has_availability != 'f'")

"""O mesmo racional vale para as outras variáveis relacionadas à disponibilidade em períodos de tempo futuro. Pode não haver disponibilidade simplesmente por que hóspedes já fizeram reserva para vários dias do calendário da acomodação ou por que o anfitrião opta por abrir disponibilidade em períodos específicos. O primeiro seria ótimo para inferir uma taxa de ocupação alta mas o segundo cenário seria péssimo, com a inferência acontecendo de maneira errada.

Portanto, vou remover essas variáveis do dataset, para não haver contaminação.
"""

# removendo has_availability, availability_30, availability_60, availability_90, availability_365
ds_cleansed.drop([HAS_AVAILABILITY, AVAILABILITY_30, AVAILABILITY_60, AVAILABILITY_90, AVAILABILITY_365], axis=1, inplace= True)

"""A variável **calendar_last_scraped** serve apenas para indicar o momento em que o calendário foi analisado para que fossem encontrados os valores de disponibilidade da acomodação em datas futuras. Não agrega na análise estatística e portanto será removida."""

# removendo calendar_last_scraped
ds_cleansed.drop([CALENDAR_LAST_SCRAPED], axis=1, inplace= True)

"""As variáveis relacionadas a quantidade absoluta de reviews, datas de primeiro e último review e frequência são: **number_of_reviews, number_of_reviews_ltm number_of_reviews_l30d, first_review, last_review e reviews_per_month**.

Entendo que a quantidade absoluta de reviews (**number_of_reviews**) pode influenciar na decisão de um hóspede em fazer uma reserva e por isso vou **manter** essa variável.

A quantidade de reviews no último ano (**number_of_reviews_ltm**) será usada para cálculo da variável alvo e por isso será **mantida**. Importante notar que todas as instâncias tem valor preenchido para essa variável e por isso não foi necessária uma ação de limpeza de instâncias.

A quantidade de reviews nos últimos 30 dias (**number_of_reviews_l30d**) pode ser um indicativo indireto de mais probabilidade de alta taxa de ocupação e por isso vou **manter** essa variável.

Vou assumir que data do primeiro review (**first_review**) é pouco relevante para taxa de ocupação e vou **remover** essa variável, ao contrário da data do último review (**last_review**), que pode ser considerado como um dado importante para decisão do hóspede em fazer a reserva, e portanto **manterei** esta última.

Já a variável de reviews por mês (**reviews_per_month**) pode ser útil para o cálculo de data de registro da acomodação na plataforma Airbnb, já que esse dado não está presente no dataset. Portanto manterei por enquanto essa variável no dataset.




"""

# removendo first_review
ds_cleansed.drop([FIRST_REVIEW], axis=1, inplace= True)

"""Existe outro grupo de variáveis relacionado ao tema de review mas desta vez elas representam de fato um valor númerico de pontuação da acomodação. São números que indicam qualitativamente como os hóspedes avaliaram a acomodação. São elas: **review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location e review_scores_value**.

É bem razoável considerarmos que a avaliação em pontos de uma acomodação influencia diretamente na sua taxa de ocupação.

No entanto, enquanto a variável review_scores_rating traz um valor numérico já sumarizado de todos os quesitos de avaliação, as outras variáveis trazem pontuações para critérios específicos.

Nesse trabalho, vou optar por usar apenas essa variável sumarizada eliminando as outras. Ficará como trabalho futuro uma análise mais aprofundada dentro das pontuações específicas se essa variável sumarizada se mostrar bastante significativa para a taxa de ocupação.

"""

# removendo review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location e review_scores_value.
ds_cleansed.drop([REVIEW_SCORES_ACCURACY, REVIEW_SCORES_CLEANLINESS, REVIEW_SCORES_CHECKIN, REVIEW_SCORES_COMMUNICATION, REVIEW_SCORES_LOCATION, REVIEW_SCORES_VALUE], axis=1, inplace= True)

"""A última variável para a entidade Acomodação é a **instant_bookable**. Ela indica se a acomodação aceita reserva imediata sem necessidade de revisão do anfitrião.

Entendo que é um fator que pode influenciar na taxa de ocupação e por isso **manterei** essa variável no dataset.

#### 4.2.2 Redução de dimensionalidade da entidade Localização da Acomodação

Removendo colunas que não contém dados.
"""

# removendo a coluna neighbourhood_group_cleansed
ds_cleansed.drop([NEIGHBOURHOOD_GROUP_CLEANSED], axis=1, inplace= True)

"""A variável **neighborhood_overview** inclui um texto escrito pelo próprio anfitrião onde ele tem a opção de descrever o bairro onde se localiza a acomodação.

Ao analisar o dataset, podemos ver que 16739 instâncias tem essa variável preenchida, ou seja, aproximadamente metade do dataset.

Eu vou **manter** essa variável **transformando-a num booleano** que indica se o texto foi preenchido ou não. Estou assumindo que o esforço de preenchimento desse texto pode influenciar na taxa de ocupação da acomodação e portanto quero considerar isso na análise do trabalho. Poderia fazer também a análise do texto envolvendo processamento de linguagem natural mas isso traria muita complexidade para esse trabalho, então descarto essa abordagem.

A variável **neighbourhood** está parcialmente preenchida no dataset e além disso, quando preenchida, carrega o nome da cidade do Rio de Janeiro. Como nossa análise já considera que todas as instâncias são do Rio de Janeiro, essa variável acaba ficando incompleta e irrelevante para a análise e por isso pode ser removida. A decisão de remoção também está baseada no fato de que existe outra variável que informa sobre bairro.
"""

# removendo a coluna neighbourhood
ds_cleansed.drop([NEIGHBOURHOOD], axis=1, inplace= True)

"""A variável **neighbourhood_cleansed** tem todas as suas instâncias com valores preenchidos e traz a informação de bairro a partir de um cálculo feito com os dados de latitude e longitude. Essa variável é muito importante para a análise proposta e será mantida.

As variáveis ***latitude*** e ***longitude*** trazem no detalhe a localização da acomodação e foram importantes para a definição clara do bairro (o dataset veio já com esse cálculo feito). No entanto, não fará parte do escopo desse trabalho fazer uma análise usando uma granularidade de localização mais precisa do que bairro. Além disso, não pretendo usar essas variáveis para nenhuma visualização com recurso de mapas. Portanto, para redução e simplificação do dataset, também opto pela remoção dessas variáveis.
"""

# removendo as colunas latitude e longitude
ds_cleansed.drop([LATITUDE, LONGITUDE], axis=1, inplace= True)

"""#### 4.2.3 Redução de dimensionalidade da entidade Anfitrião

**Removendo** **host_id** e **host_url** por que não contribuem estatisticamente para atuar com o problema proposto.
"""

# removendo as colunas host_id e host_url
ds_cleansed.drop([HOST_ID, HOST_URL], axis=1, inplace= True)

"""A coluna **host_name** contém o nome de uma pessoa e por questões de privacidade de dados, estou **removendo** essa variável.

Esse dado até poderia ser considerado como influenciador do processo de decisão do hóspede uma vez que nomes mais familiares para a nacionalidade da pessoa podem soar mais próximos e amigáveis, ou mesmo trazer um entendimento de que falam o mesmo idioma. No entanto, opto por descartar para atender requisito de privacidade.



"""

# removendo a coluna host_name
ds_cleansed.drop([HOST_NAME], axis=1, inplace= True)

"""Vou **manter** a variável **host_since** pelo entendimento de que anfitriões mais antigos podem ter mais habilidade nas decisões de gestão da acomodação fazendo com que a taxa de ocupação melhore.

Verificando o preenchimento de **host_location** e **host_neighbourhood**.
"""

ds_cleansed['host_location'].value_counts().head(15)

ds_cleansed['host_neighbourhood'].value_counts().head(30)

"""Vou **manter, por enquanto**, **host_location** e **host_neighbourhood** por entender que uma proximidade do anfitrião com a localização da acomodação pode trazer facilidades logísticas de administração e isso influenciaria positivamente na taxa de ocupação.

A variável **host_about** é um descrição preenchida pelo próprio anfitrião sobre ele ou ela mesmos. Cerca de metade das instâncias tem valores preenchidos para esta variável. Entendo que isso funciona como uma apresentação e melhora as relações humanas entre hóspede e anfitrião, influenciando na decisão do hóspede.

Mais uma vez, eu poderia usar uma técnica de processamento de linguagem natural para avaliar o preenchimento desse campo mas vou fazer apenas uma diferenciação entre estar preenchido ou não. Portanto, **mantenho** essa variável no dataset.

A variável **host_response_time**	é uma variável categórica ordenada e as variáveis **host_response_rate**	e **host_acceptance_rate** são variáveis numéricas. Considero todas elas relevantes para influenciar na taxa de ocupação da acomodação e por isso vou **mantê-las**.

Segundo o próprio Airbnb, Superhosts [são anfitriões de mais alto desempenho](https://www.airbnb.com.br/help/article/829). A variável **host_is_superhost** é um booleano que traz essa informação e provavelmente tem influência na taxa de ocupação. Vou **manter** essa variável no dataset.

As variáveis **host_thumbnail_url**	e **host_picture_url** estão preenchidas para praticamente todas as instâncias do dataset. A variável **host_has_profile_pic** deriva de host_picture_url. Por questões de simplificação e também por entender que uma foto do anfitrião envolve questões de privacidade de dados, opto por **remover** essas variáveis.
"""

# removendo as colunas host_thumbnail_url, host_picture_url e host_has_profile_pic
ds_cleansed.drop([HOST_THUMBNAIL_URL, HOST_PICTURE_URL, HOST_HAS_PROFILE_PIC], axis=1, inplace= True)

"""As variáveis **host_listings_count**, **host_total_listings_count**, **calculated_host_listings_count**, **calculated_host_listings_count_entire_homes**, **calculated_host_listings_count_private_rooms** e **calculated_host_listings_count_shared_rooms** trazem dados sobre a quantidade e o tipo de acomodações sob responsabilidade de um anfitrião. As duas primeiras são informações provenientes do próprio Airbnb enquanto as restantes são calculadas no processo de scrapping de dados do site Inside Airbnb.

Eu vou considerar que essa métrica é relevante para influenciar em taxa de ocupação supondo que a habilidade do anfitrião melhora com acomodações a mais para serem administradas mas passa a piorar a partir de algum ponto de inflexão, se não existir uma equipe para trabalhar em conjunto na administração.

Por questões de simplificação, opto por selecionar e **manter** apenas a variável **host_total_listings_count**, proveniente do Airbnb, como a representante desse conjunto de dados.

Então eu removo as variáveis restante desse conjunto.





"""

# removendo as colunas host_listings_count, calculated_host_listings_count, calculated_host_listings_count_entire_homes, calculated_host_listings_count_private_rooms e calculated_host_listings_count_shared_rooms
ds_cleansed.drop([HOST_LISTINGS_COUNT, CALCULATED_HOST_LISTINGS_COUNT, CALCULATED_HOST_LISTINGS_COUNT_ENTIRE_HOMES, CALCULATED_HOST_LISTINGS_COUNT_PRIVATE_ROOMS, CALCULATED_HOST_LISTINGS_COUNT_SHARED_ROOMS], axis=1, inplace= True)

"""Analisando host_verifications:"""

ds_cleansed[HOST_VERIFICATIONS].value_counts()

"""Por questões de simplificação, vou assumir que essa variável não é relevante para influenciar na taxa de ocupação e portanto, opto por removê-la."""

# removendo a coluna host_verifications
ds_cleansed.drop([HOST_VERIFICATIONS], axis=1, inplace= True)

"""Analisando 	host_identity_verified"""

ds_cleansed[HOST_IDENTITY_VERIFIED].value_counts()

"""A [verificação de identidade](https://www.airbnb.com/help/article/1237#section-heading-9-0) é um processo importante para a plataforma Airbnb e as pessoas que passaram por esse processo ganham um badge, o que aumenta o seu nível de credibilidade. Portanto, vou considerar a variável **host_identity_verified** como relevante para taxa de ocupação e portanto, opto por **mantê-la**.

#### 4.2.4 Redução de dimensionalidade da entidade Outros

A entidade Outros é apenas um agrupamento de variáveis que considerei não significativas para influenciar na taxa de ocupação e que tem mais relação com o processo de captura de dados.

Com isso, opto por removê-las do dataset.
"""

# removendo as colunas scrape_id, last_scraped e source
ds_cleansed.drop([SCRAPE_ID, LAST_SCRAPED, SOURCE], axis=1, inplace= True)

"""#### 4.2.5 Análise após ações de redução de dimensionalidade

Verificando agora como ficaram as dimensões do dataset após as ações de limpeza.
"""

# Análise de dimensões do dataset

print('Quantidade de linhas e quantidade de colunas:' , ds_cleansed.shape)

"""Notem que a quantidade de instâncias diminiui devido à ação de limpeza feita com as acomodações com reservas indisponíveis.

A quantidade de variáveis também diminuiu significativamente, saindo de um total de 75 para 30.

A variável listing_url ainda foi mantida para ser utilizada caso alguma acomodação tenha que ser identificada unicamente. Outras variáveis também foram mantidas para possível uso em conversão de dados.

Numa visão tabular e de acordo com a análise baseada em entidades, o dataset ficou assim:

<br>

Acomodação         | Localização da Acomodação | Anfitrião
-------------------|------------------------|------------------
listing_url*       | neighborhood_overview  | host_since                
property_type      | neighbourhood_cleansed | host_location
room_type          |                        | host_about           
accommodates       |                        | host_response_time
bathrooms_text     |                        | host_response_rate
bedrooms           |                        | host_acceptance_rate                 
beds               |                        | host_is_superhost  
amenities          |                        | host_neighbourhood
price              |                        | host_total_listings_count
minimum_nights     |                        | host_identity_verified
maximum_nights     |                        |          
number_of_reviews  |                        |        
number_of_reviews_ltm |                     |       
number_of_reviews_l30d |                    |     
last_review        |                        |    
review_scores_rating |                      |  
instant_bookable   |                        |           
reviews_per_month  |                        |

E agora mais uma vez a visão matricial do dataset.
"""

ms.matrix(ds_cleansed)

"""Notem que já não existem as colunas sem dados e que a quantidade de colunas é bem menor, permitindo a geração do gráfico com a presença do nome de cada variável. Fica mais claro de visualizar também as variáveis parcialmente preenchidas.

### 4.3 Tratamento de missings, limpeza e conversão de dados

Essa seção visa agrupar todas as ações a serem executadas no dataset para garantir a completude e a qualidade do mesmo. Alguns dos problemas que devem ser tratados nesta etapa do pré-processamento podem ser: alinhamento de unidades de medida, preenchimento de valores faltantes, acerto de valores inconsistentes ou adaptação de intervalores de valores. Usarei técnicas de *feature engineering* como extração de características e construção de recursos para capturar informações importantes.


Criarei um novo dataset para essa etapa, mantendo os datasets anteriores inalterados a partir daqui.
"""

# criação de um novo dataset para executar as operações de tratamento

# recuperando os nomes das colunas
col_cleansed = list(ds_cleansed.columns)

# o novo dataset irá conter todas as colunas do dataset original
ds_treated = ds_cleansed[col_cleansed[:]]

"""#### 4.3.1 Análise inicial e planejamento

Repito aqui uma análise básica de metadados para percepções introdutórias.
"""

# Exibir os tipos de dados de todas as variáveis
print(ds_treated.dtypes)

"""Já podemos perceber grande oportunidade de ajuste de tipagem de dados."""

# Analisando a completude de dados de cada variável
ds_treated.count()

"""Também podemos perceber a necessidade de ações para complementação do dataset eliminando a existência de valores nulos.

Repito aqui a visão tabular de entidades pois farei as ações de ajuste no dataset nesta seção organizando essas ações por entidade.

<br>

Acomodação         | Localização da Acomodação | Anfitrião
-------------------|------------------------|------------------
listing_url*       | neighborhood_overview  | host_since                
property_type      | neighbourhood_cleansed | host_location
room_type          |                        | host_about           
accommodates       |                        | host_response_time
bathrooms_text     |                        | host_response_rate
bedrooms           |                        | host_acceptance_rate                 
beds               |                        | host_is_superhost  
amenities          |                        | host_neighbourhood
price              |                        | host_total_listings_count
minimum_nights     |                        | host_identity_verified
maximum_nights     |                        |          
number_of_reviews  |                        |        
number_of_reviews_ltm |                     |       
number_of_reviews_l30d |                    |     
last_review        |                        |    
review_scores_rating |                      |  
instant_bookable   |                        |           
reviews_per_month  |                        |

#### 4.3.2 Tratamentos na entidade Acomodação

##### Property Type e Room Type

Analisando valores da variável property_type.
"""

# verificando valores para variável property_type
ds_treated[PROPERTY_TYPE].value_counts()

"""Analisando valores da variável room_type."""

# verificando valores para variável room_type
ds_treated[ROOM_TYPE].value_counts()

"""Ao observar que a variável property_type traz informações mais detalhadas sobre as acomodações e a variável room_type traz informações mais alinhadas com uma visão de categorias e mais simples, opto por remover a coluna property_type, usando apenas room_type. As descrições presentes em property_type, quando específicas, atendem as necessidades de hóspedes que buscam experiências especiais. Esses dados entram num nível de granularidade não necessário para o escopo desse trabalho. As categorias presentes em room_type são mais bem alinhadas com o que se busca nesse notebook."""

# removendo a coluna property_type
ds_treated.drop([PROPERTY_TYPE], axis=1, inplace= True)

"""##### Accomodates

Essa coluna está totalmente preenchida e o tipo de dado está definido corretamente, como um dado numérico. Por enquanto não farei nenhuma ação de ajuste nessa variável.

##### Bathrooms

Segundo o dicionário de dados do site Inside Airbnb, o Airbnb mudou o campo de banheiros para o formato texto. A coluna bathrooms_text traz essa informação.
"""

# verificando valores para variável bathrooms_text
ds_treated['bathrooms_text'].value_counts()

"""Notem que existem instâncias do dataset que informam números não inteiros para indicar quantidade de banheiros e também há a indicação sobre banheiros compartilhados. Após uma breve pesquisa na internet ([link](https://airbnbtales.com/shared-bathroom/#:~:text=The%20%E2%80%9C.,be%20handy%20for%20quick%20visits.)), encontrei respostas indicando que esse é um conceito do mercado de imóveis de algumas regiões, como nos EUA. Uma indicação de 1.5, por exemplo, indicaria que existe um banheiro completo com chuveiro/banheira, vaso sanitário e pia e mais outro apenas com vaso sanitário e pia. Também encontrei links que sinalizam dificuldades dos anfitriões em informar corretamente esse dado, justamente por não conhecer o conceito ([link](https://community.withairbnb.com/t5/Help/Shared-bathroom-classification-1-5-or-ZERO/m-p/280614)).

Notem também que não seria correto eu fazer uma transformação direta dessa variável para numérico pois perderia bastante significado semântico do valor do campo. Uma acomodação do tipo quarto, que tem um banheiro exclusivo (suíte) tem atratividade bastante diferente de um quarto que oferece ao hóspede apenas um banheiro compartilhado.

Farei uma transformação nessa variável para buscar representar a disponibilidade de banheiros por hóspede e também considerarei o fato de ser compartilhado ou não, usando uma simplificação. Quero chegar num valor numérico que seja mais representativo para a atratividade de uma acomodação considerando a oferta de banheiros.

Quando não houver a palavra shared, vou assumir que a quantidade de banheiros declarada vai ser dividida pela quantidade de hóspedes aceitos. Sei que nem sempre as reservas são feitas usando o total de hóspedes aceitos mas vou assumir que essa relação é quase sempre adequada e estável.

Quando houver a palavra shared, vou fazer uma conta similar à anterior, com a diferença de que vou adicionar 1 ao valor total de hóspedes aceitos, para afetar negativamente o resultado numérico da equação nesse caso.

Alguns exemplos:

Banheiros | Hóspedes | Resultado
----------| -------- | ---------
1 bath | 1 | 1
1 bath | 2 | 0.5
1 bath | 3 | 0.33
2 baths | 2 | 1
2 baths | 4 | 0.5
1 shared bath | 1 | 0.5
1 shared bath | 2 | 0.33

As regras de negócios definidas acima foram inseridas na função get_bathrooms_index já declarada.

Vou criar uma nova coluna chamada baths_index que será usada para armazenar os valores numéricos segundo as regras definidas nessa seção e também vou criar uma nova coluna chamada baths_category que simplificará a leitura de categoria de banheiro, além de permitir verificar se aparecerá alguma diferença na métrica da taxa de ocupação de acordo com essas categorias.

Criando a nova coluna com o índice de banheiros.
"""

# criando uma nova coluna no dataset para registrar variável que represente o índice de banheiros
ds_treated[BATHS_INDEX] = ds_treated.apply(get_bathrooms_index, axis=1)

"""Criando uma nova coluna com as categorias de banheiro."""

# criando uma nova coluna no dataset para registrar variável que represente a categoria de banheiro
ds_treated[BATHS_CATEGORY] = ds_treated.apply(get_bathrooms_category, axis=1)

"""Agora faço uma análise estatística nas colunas recém criadas."""

ds_treated[BATHS_INDEX].describe()

ds_treated[BATHS_CATEGORY].value_counts()

"""Agora vou remover a coluna bathrooms_text."""

# removendo a coluna bathrooms_text
ds_treated.drop([BATHROOMS_TEXT], axis=1, inplace= True)

"""##### Bedrooms

Analisando valores da coluna Bedrooms.
"""

# verificando valores para variável bedrooms
ds_treated[BEDROOMS].value_counts()

# verificando nulos da variável bedrooms
ds_treated[BEDROOMS].info()

"""Minha hipótese é que acomodações do tipo quarto não precisam informar quantidade de quartos e por isso várias instâncias não tem esse valor preenchido."""

# revisitando a quantidade de cada tipo de acomodação
ds_treated[ROOM_TYPE].value_counts()

# verificando a quantidade de quartos por tipo de acomodação quando existem valores preenchidos
contagem_nao_nulos = ds_treated.groupby(ROOM_TYPE)[BEDROOMS].count()

# verificando a quantidade de quartos por tipo de acomodação quando não existem valores preenchidos
contagem_nulos = ds_treated.groupby(ROOM_TYPE)[BEDROOMS].apply(lambda x: x.isnull().sum())

# Exibir os resultados
print("Contagem de bedrooms não nulos por categoria:")
print(contagem_nao_nulos)

print("\nContagem de bedrooms nulos por categoria:")
print(contagem_nulos)

"""Notem que a minha hipótese é parcialmente verdadeira visto que existem valores nulos da coluna bedrooms para todas as categorias.

Após analisar esses números, entendo que vale a pena preencher os valores não nulos com uma regra específica. Vou preencher com valor 1 para todas as categorias de room_type, exceto para a categoria Shared room. Quando a acomodação for dessa categoria, vou preencher com valores que representem frações, para o número também carregar a informação de que o quarto está sendo dividido. A fração vai ser calculada usando a quantidade de hóspedes.

A função que aplica as regras descritas acima já foi declarada.

Criando uma nova coluna com os valores ajustados para quartos.
"""

# criando uma nova coluna no dataset para registrar variável que represente o índice de banheiros
ds_treated[BEDROOMS_ADPT] = ds_treated.apply(get_bedrooms_adjusted_value, axis=1)

"""Analisando valor originais para quartos."""

ds_treated[BEDROOMS].value_counts()

"""Analisando novos valores."""

ds_treated[BEDROOMS_ADPT].value_counts()

"""Notem o incremento significativo na quantidade relativa a 1 quarto e o aparecimento de valores fracionados, dada a regra adotada.

Após comparar as colunas, vou remover a original.
"""

# removendo a coluna bedrooms
ds_treated.drop([BEDROOMS], axis=1, inplace= True)

"""##### Beds

Analisando coluna beds.
"""

# verificando valores para variável beds
ds_treated[BEDS].value_counts()

# verificando nulos da variável beds
ds_treated[BEDS].info()

"""Existem algumas instâncias com valor nulo para quantidade de camas. Vou analisar a média de camas por room_type."""

# categorias que usarei para analisar a média de camas
#categorias = ['Entire home/apt','Private room','Shared room','Hotel room']
categorias_acomodacoes = ds_treated[ROOM_TYPE].unique()

# vou iterar pelas categorias e verificar a média de camas para cada uma
for cat in categorias_acomodacoes:
  print(f'Categoria: {cat}')
  df_filtrado = ds_treated[ds_treated[ROOM_TYPE].isin([cat])]
  estatisticas = df_filtrado[BEDS].describe()
  print(estatisticas)

"""Como são valores diferentes de média de acordo com o tipo de acomodação, vou usar essas médias específicas na hora de preencher os nulos.

Implementando a lógica de programação de acordo com as regras propostas.
"""

#dicionario que vai armazenar as médias de quantidade de camas por tipo de acomodação
media_camas_dict = {}

# vou iterar pelos tipos e verificar a média de camas para cada uma
for tipo in categorias_acomodacoes:
  df_filtrado = ds_treated[ds_treated[ROOM_TYPE].isin([tipo])]
  estatisticas = df_filtrado[BEDS].describe()
  media_camas_dict.update({tipo : round(estatisticas.loc['mean'])})

print(f'O dicionário de médias de camas por tipo de acomodação ficou assim: {media_camas_dict}')

# criando uma nova coluna no dataset para registrar variável que represente o valor ajustado de camas
ds_treated[BEDS_ADPT] = ds_treated.apply(get_beds_adjusted_value_by_roomtype, axis=1, medias=media_camas_dict)

"""Comparando a coluna original com a nova coluna criada."""

ds_treated[BEDS].value_counts()

ds_treated[BEDS_ADPT].value_counts()

"""Notem que apenas as quantidades 2, 3 e 5, presentes no dicionário de médias, que sofreram incremento.

Apagando a coluna original.
"""

# removendo a coluna beds
ds_treated.drop([BEDS], axis=1, inplace= True)

"""##### Amenities

Analisando o conteúdo da variável amenities, que representa comodidates existentes na acomodação.
"""

# aumentando a visibilidade do conteúdo
pd.set_option('display.max_colwidth', None)
ds_treated[AMENITIES].head()

"""Notem que o conteúdo indica uma lista de itens de grande variedade. Não vou entrar no mérito de fazer uma análise fina das comodidades oferecidas, pois isso aumentaria muito o escopo desse trabalho. Para simplificar, vou fazer um contagem simples de quantidade de comodidades oferecida."""

# criando uma nova coluna no dataset para registrar a quantidade de comodidades
ds_treated[AMENITIES_COUNT] = ds_treated.apply(get_amenities_count, axis=1)

"""Analisando se os valores foram preenchidos corretamente."""

# Exibindo apenas amenities e amenities_count
ds_treated[[AMENITIES,AMENITIES_COUNT]].head()

"""Removendo a coluna original."""

# removendo a coluna amenities
ds_treated.drop([AMENITIES], axis=1, inplace= True)

# restaurando valor padrão
pd.set_option('display.max_colwidth', 50)

"""##### Price

Analisando a variável price.
"""

ds_treated[PRICE].head()

# Tratando a coluna 'price' e dessa vez substituindo diretamente na mesma coluna
ds_treated[PRICE] = ds_treated[PRICE].str.replace('$', '').str.replace(',', '').astype(float)

ds_treated[PRICE].head()

"""##### Minimum_nights e Maximum_nights

Análise simples apenas para verificar a necessidade de alguma filtragem.
"""

ds_treated[MINIMUM_NIGHTS].describe()

ds_treated[MAXIMUM_NIGHTS].describe()

"""Não farei nenhuma ação de tratamento nesses dados pois estão completos e já são tratados como numéricos. Não existe nenhuma acomodação com valores incompatíveis, como por exemplo uma definição de número máximo de noites como 0 (zero).

##### Number_of_reviews, Number_of_reviews_ltm, Number_of_reviews_l30d, Last_review, reviews_per_month e review_scores_rating

Analisando dados relacionados às avaliações das acomodações: quantidade de reviews, data e pontuação.
"""

# variável que agrega as colunas relacionados ao tema de reviews
colunas_review = [NUMBER_OF_REVIEWS, NUMBER_OF_REVIEWS_LTM, NUMBER_OF_REVIEWS_L30D, LAST_REVIEW, REVIEWS_PER_MONTH, REVIEW_SCORES_RATING]

ds_treated[colunas_review].head(15)

ms.matrix(ds_treated[colunas_review])

"""Notem que existem acomodações que nunca receberam uma avaliação e quando isso acontece não existem valores para data de última avaliação, revisões por mês e a própria pontuação média.

Vou verificar a quantidade de ocorrências de valores nulos para essas variáveis.
"""

ds_treated[colunas_review].info()

"""**Reflexões sobre como tratar essa quantidade considerável de instâncias com valores nulos.**

23630 instâncias sem valores nulos representam um universo de 76% do total de 30940. Eu poderia abrir mão dos 24% para garantir um dataset preenchido para essas variáveis mas ao optar por isso, estaria perdendo informação sobre acomodações com total rejeição. Será que existe, por exemplo, um bairro do Rio de Janeiro com disponibilidade de acomodações mas onde nunca alguém fez um review tendo ficado hospedado por lá? Ou mesmo nenhum hóspede quis se hospedar por lá, mesmo com acomodações disponíveis. Existem características de uma acomodação que são definitivas para implicar em rejeição por parte dos hóspedes?

Por outro lado, ao não abrir mão dessas instâncias com valores nulos, como vou tratar esses dois pontos?

*   Eu gostaria de calcular a data em que uma acomodação ficou disponível na plataforma fazendo uma conta com as variáveis number_of_reviews e reviews_per_month. Se a primeira tem valor 10 e a segunda tem valor 1, eu poderia inferir que a acomodação existe há 10 meses, por exemplo. Não tendo valor para reviews_per_month, eu não consigo fazer essa conta. Saber a data de início da disponibilização da acomodação é importante para eu filtrar acomodações com menos de 12 meses pois vou usar a variável de reviews nos últimos 12 meses como variável alvo.
*   Eu também gostaria de usar valores significativos de pontuação das acomodações (review_scores_rating) como uma variável de influência no dataset. Como tratar então valores nulos para essa variável? Não posso preencher com zero por que a semântica desse valor é bem diferente do valor não existir.

**Ponto de decisão**: Para caminhar no sentido de simplificação e por entender que essa ação implica apenas em perda parcial de informação, vou optar por remover as instâncias com valores nulos para essas colunas. Elas representam 24% do dataset e a informação sobre rejeição de acomodações vai seguir presente no conjunto de dados, com a existência de número de reviews baixo e pontuações baixas.
"""

# removendo instâncias com valores nulos para colunas de dados de review
ds_treated.dropna(subset=colunas_review, inplace=True)

#conferindo a remoção
ds_treated[colunas_review].info()

"""Vou converter a coluna last_review num dado do tipo datetime para poder executar operações adequadas com esse dado."""

#convertendo em datetime
ds_treated[LAST_REVIEW] = pd.to_datetime(ds_treated[LAST_REVIEW], format='%Y-%m-%d')

#conferindo o novo tipo de dado
ds_treated[colunas_review].info()

"""Vou adicionar uma nova coluna que vai conter a quantidade de meses desde que a acomodação foi registrada na plataforma Airbnb. Vou calcular esse valor usando valores presentes nas colunas reviews_per_month e number_of_reviews."""

#analisando valores presentes
ds_treated[[NUMBER_OF_REVIEWS, REVIEWS_PER_MONTH]].head()

# criando uma nova coluna no dataset para registrar a quantidade de meses
ds_treated[MONTHS_SINCE_REGISTERED] = ds_treated.apply(calcula_meses_desde_registro, axis=1)

ds_treated[[NUMBER_OF_REVIEWS, REVIEWS_PER_MONTH, MONTHS_SINCE_REGISTERED]].head()

#analisando dados da nova coluna criada
ds_treated[MONTHS_SINCE_REGISTERED].describe()

"""Notem que existe uma acomodação com 200 meses de registrada, ou seja, mais de 16 anos e meio. A plataforma Airbnb foi fundada em 2008 o que dá 15 anos se consideramos o momento atual, ano 2023. Além disso, ela opera no Brasil desde 2012, o que dá aproximadamente 11 anos. Parece haver alguma inconsistência no dado mas não vou eliminar essas instâncias no escopo desse trabalho.

Vou agora analisar quantas acomodações tem menos de 12 meses.
"""

menos_de_doze = (ds_treated[MONTHS_SINCE_REGISTERED] < 12).sum()

print(f'Quantidade menor do que 12: {menos_de_doze}')

"""Como quero usar a variável de quantidade de reviews nos últimos 12 meses como a variável alvo, preciso remover do dataset aquelas acomodações com existência na plataforma menor do que esse período."""

# removendo linhas com menos de 12 meses
ds_treated = ds_treated[ds_treated[MONTHS_SINCE_REGISTERED] >= 12]

#analisando novamente os dados da nova coluna criada
ds_treated[MONTHS_SINCE_REGISTERED].describe()

"""Como foram retiradas as instâncias com menos de 12 meses, os valores dos quartis e a média aumentaram significativamente

##### Instant_bookable

Analisando a última coluna da entidade Acomodação e aplicando os devidos tratamentos.
"""

# analisando primeiros valores
ds_treated[INSTANT_BOOKABLE].head(10)

# verificando distribuição
ds_treated[INSTANT_BOOKABLE].value_counts()

#convertendo para tipo bool
ds_treated[INSTANT_BOOKABLE] = ds_treated[INSTANT_BOOKABLE].map({'t': True, 'f': False})

#analisando resultado do ajuste de tipagem
ds_treated[INSTANT_BOOKABLE].head(10)

"""#### 4.3.3 Tratamento na entidade Localização da Acomodação

Essa entidade ficou com apenas dois atributos e isso simplificou as ações de tratamento.

##### Neighbourhood_overview

Analisando a coluna neighbourhood_overview.
"""

ds_treated[NEIGHBORHOOD_OVERVIEW].head()

"""Como mencionado em seções anteriores, para algumas variáveis eu vou adotar a estratégia de considerar que o fato dela ter valor preenchido já serve de indicativo para fator de influência. Não farei uma análise no dado em si.

A variável neighborhood_overview se encaixa nesse cenário.

Criarei uma coluna nova preenchendo-a com valores booleanos correspondentes e depois excluirei a coluna original.
"""

# criando uma coluna nova e convertando para booleano para indicar que a variável neighbourhood_overview tem valor preenchido
ds_treated[HAS_NEIGHBOURHOOD_OVERVIEW] = ds_treated[NEIGHBORHOOD_OVERVIEW].notna()

#conferindo o resultado nas primeiras linhas
ds_treated[[NEIGHBORHOOD_OVERVIEW, HAS_NEIGHBOURHOOD_OVERVIEW]].head()

# removendo coluna original neighborhood_overview
ds_treated.drop([NEIGHBORHOOD_OVERVIEW], axis=1, inplace= True)

"""##### Neighbourhood_cleansed

Essa coluna contém nomes dos bairros que foram calculados a partir dos dados de latitude e longitude, cálculo esse feito pelo site Inside Airbnb. A cidade do Rio de Janeiro tem 164 bairros, segundo a [Wikipedia](https://pt.wikipedia.org/wiki/Lista_de_bairros_da_cidade_do_Rio_de_Janeiro).

Vou agora analisar a coluna neighbourhood_cleansed do dataset.
"""

ds_treated[NEIGHBOURHOOD_CLEANSED].info()

"""Notem a ausência de valores nulos para essa coluna. Existem valores preenchidos para todas as 16322 entradas."""

# verificando valores únicos para bairros para comparar com total de bairros da cidade
print("Quantidade de bairros:", ds_treated[NEIGHBOURHOOD_CLEANSED].nunique())

"""Como existem 132 bairros presentes e a cidade tem 164 bairros, posso concluir que no estado atual do meu dataset, nem todo bairro do Rio de Janeiro está presente.

Vou verificar a contagem por bairro.
"""

#listando os bairros presentes, em ordem de quantidade de acomodações
ds_treated[NEIGHBOURHOOD_CLEANSED].value_counts()

# fazendo agora a mesma listagem mas ordenando pelos nomes dos bairros, para tentar identificar uma possível ocorrência de bairros repetidos escritos de maneira similar.
ds_treated[NEIGHBOURHOOD_CLEANSED].value_counts().sort_index()

"""Numa análise da lista, não parece acontecer o cenário mencionado, de bairros duplicados. É interessante notar a presença na lista de alguns bairros não tão conhecidos, como [Higienópolis](https://pt.wikipedia.org/wiki/Higien%C3%B3polis_(bairro_do_Rio_de_Janeiro)) (não é de São Paulo) e [Tauá](https://pt.wikipedia.org/wiki/Tau%C3%A1_(bairro)).

Vou criar uma variável com a lista de bairros para usar numa seção subsequente.
"""

# criando variável com a lista de bairros
bairros_referencia = sorted(ds_treated[NEIGHBOURHOOD_CLEANSED].unique())

"""#### 4.3.4 Tratamento na entidade Anfitrião

A entidade Anfitrião engloba as variáveis host_since, host_location, host_about, host_response_time, host_response_rate, host_acceptance_rate, host_is_superhost, host_neighbourhood, host_total_listings_count e host_identity_verified.

Analisarei e farei os devidos tratamentos nessas variáveis nas próximas seções. Para começar, uma visão de matriz apenas das variáveis de Anfitrião.
"""

#lista com colunas da entidade Anfitrião
colunas_host = [HOST_SINCE, HOST_LOCATION, HOST_ABOUT, HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE, HOST_IS_SUPERHOST, HOST_NEIGHBOURHOOD, HOST_TOTAL_LISTINGS_COUNT, HOST_IDENTITY_VERIFIED]

# matriz apenas dos dados dessa entidade
ms.matrix(ds_treated[colunas_host])

"""Notem a presença de valores nulos para boa parte das variáveis envolvidas.

##### Host_since

A variável host_since indica em que momento o anfitrião se registrou na plataforma, seja para ofertar acomodações, seja para se hospedar. Vou manter esse dado no dataset por considerar que mais tempo de relação com a plataforma pode indicar mais habilidade de gerir estadias e isso levar a uma melhor taxa de ocupação.

Análise inicial da variável.
"""

# verificando nulos da variável host_since
ds_treated[HOST_SINCE].info()

"""Todas as instâncias contém dados e o tipo do dado é object.

Agora vou analisar o conteúdo em si e seu formato.
"""

ds_treated[HOST_SINCE].head()

"""Vou modificar o tipo do dado para datetime."""

#convertendo em datetime
ds_treated[HOST_SINCE] = pd.to_datetime(ds_treated[HOST_SINCE], format='%Y-%m-%d')

"""Analisando agora o resultado."""

ds_treated[HOST_SINCE].head()

"""##### Host_location e host_neighbourhood

Essas variáveis indicam a localização do anfitrião em níveis de granularidade diferentes. O dicionário de dados fornecido pelo site Inside Airbnb informa que host_location contém uma informação de localização dos anfitriãos declaradas por eles mesmos e não há uma descrição sobre host_neighbourhood.  Minha hipótese é que anfitriões mais próximo dos locais das acomodações ganham em poder de gestão e logística e isso levaria a uma maior taxa de ocupação.

Analisando agora os valores presentes nessas duas variáveis para poder decidir como tratar esses dados.
"""

# informações sobre as duas variáveis
ds_treated[[HOST_LOCATION, HOST_NEIGHBOURHOOD]].info()

"""Existe uma quantidade significativa de nulos, principalmente na variável host_neighbourhood."""

# analisando valores presentes
ds_treated[[HOST_LOCATION, HOST_NEIGHBOURHOOD]].head(30)

"""Já podemos notar algumas inconsistências nessa lista de 30 ocorrências. Um exemplo é o caso do anfitrião que informa localização sendo a cidade de San Diego, na Califórnia e o bairro do Joá, no Rio de Janeiro.

Vamos analisar agora as ocorrências em que aparecem valores nulos em pelo menos um desses dois campos.
"""

# Filtrar e imprimir apenas as linhas que contêm pelo menos um valor nulo em qualquer coluna
df_filtrado = ds_treated[[HOST_LOCATION, HOST_NEIGHBOURHOOD]][ds_treated[[HOST_LOCATION, HOST_NEIGHBOURHOOD]].isnull().any(axis=1)]
print(df_filtrado.head(30))

"""Podemos notar que esse dado está incompleto e apresenta inconsistências. No entanto, como quero utilizá-lo com o propósito de qualificar anfitriões que estão mais próximos do local das acomodações sendo ofertadas, vou adotar uma estratégia de enriquecimento e ajuste dos dados. Quero no final chegar numa estrutura de dados que entregue essa visão de proximidade do anfitrião com o local da acomodação. A intenção é chegar numa variável categórica que entregue esse valor.

Vou analisar com um pouco mais de detalhe os valores presentes.


"""

#contagem de ocorrências dos valores presentes em host_location
ds_treated[HOST_LOCATION].value_counts().head(10)

"""O caso do Rio de Janeiro é especial por que a cidade e o estado tem o mesmo nome então isso traz uma dificuldade para separamos claramente anfitriões que vivem na cidade do Rio, usando os dados presentes.

---



"""

#valores presentes na coluna host_neihbourhood
bairros_anfitriao = ds_treated[HOST_NEIGHBOURHOOD].unique()
print(bairros_anfitriao)

"""Numa leitura na diagonal, podemos notar que existem nessa lista bairros da cidade do Rio de Janeiro e outros desconhecidos.

Depois de analisar essas duas variáveis, proponho as seguintes regras para o tratamento desses dados:

1.   Assumir que os valores **"Rio de Janeiro, Brazil" e "Rio, Brazil"** indicam anfitriões que vivem na cidade do Rio de Janeiro e todos os outros valores indicam anfitriões que vivem fora da cidade
2.   Assumir que o dado de host_location é mais confiável do que o dado host_neighbourhood pois o primeiro contém descrição presente no dicionário do site Inside Airbnb enquanto o segundo não contém, além desse segundo ter maior volume de nulos na coluna.
3.  No cenário de nulos presentes, fazer enriquecimento de dados apenas nos casos em que host_location está nulo, host_neighbourhood está preenchido e o valor presente é de um bairro existente na lista de bairros informados na entidade Localização da Acomodação.


Notem que nessa regra 3 eu estou considerando a lista de bairros da variável neighbourhood_cleansed como aquela que contém informações mais confiáveis. Esses dados foram preenchidos pelo próprio site Inside Airbnb a partir dos dados de latitude e longitude.


Dadas essas regras, proponho uma lógica que entregue as seguintes categorias:

*   Anfitrião localizado na cidade do Rio de Janeiro, no bairro da acomodação (Very Close)
*   Anfitrião localizado na cidade do Rio de Janeiro (Relatively Close)
*   Anfitrião não localizado na cidade do Rio de Janeiro (Far)
*   Sem informação de localização do anfitrião (No Information)

Primeiramente vou complementar o valor de host_location com a cidade do Rio de Janeiro para as ocorrências nulas com bairros reconhecidos e vou verificar com dois exemplos se a lógica funcionou como esperado.
"""

# criando uma nova coluna no dataset para registrar se o host vive no Rio de Janeiro
ds_treated[HOST_LOCATION_ADJUSTED] = ds_treated.apply(fill_in_rio, axis=1, bairros_referencia=bairros_referencia)

#obtendo um dataset filtrado para os casos com null para testar o preenchimento
df_filtrado = ds_treated[[HOST_LOCATION, HOST_NEIGHBOURHOOD, HOST_LOCATION_ADJUSTED]][ds_treated[[HOST_LOCATION, HOST_NEIGHBOURHOOD, HOST_LOCATION_ADJUSTED]].isnull().any(axis=1)]

#verificando se o valor não foi preenchido para um bairro que não é do Rio de Janeiro
print(df_filtrado.loc[df_filtrado[HOST_NEIGHBOURHOOD] == 'Iguaba Pequena'])

#verificando se o valor foi preenchido para um bairro que é do Rio de Janeiro
print(df_filtrado.loc[df_filtrado[HOST_NEIGHBOURHOOD] == 'Vila Isabel'])

"""Agora vou implementar a lógica que cria as categorias de proximidade do anfitrião."""

# criando uma nova coluna no dataset para registrar se o host vive no Rio de Janeiro
ds_treated[HOST_PROXIMITY] = ds_treated.apply(host_proximity, axis=1)

#conferindo o resultado nas primeiras linhas
ds_treated[[HOST_LOCATION, HOST_LOCATION_ADJUSTED, HOST_NEIGHBOURHOOD, NEIGHBOURHOOD_CLEANSED, HOST_PROXIMITY]].head(50)

"""Notem que o resultado está de acordo com as regras estabelecidas e alguns casos chamam a atenção dada a inconsistência. Uma acomodação indica que o anfitrião está em Zug, Switzerland (fora do Brasil) e ao mesmo tempo em Copacabana, bairro do Rio de Janeiro. Como definimos como regra que vamos confiar mais no dado indicado em host_location, consideramos que nesse caso o anfitrião está longe da acomodação.

Agora vou remover todas as colunas de localização do host mantendo apenas a nova coluna categórica criada.
"""

# removendo colunas originais e temporárias de localização do host
ds_treated.drop([HOST_LOCATION, HOST_NEIGHBOURHOOD, HOST_LOCATION_ADJUSTED], axis=1, inplace= True)

"""##### Host_about

Essa variável contém uma apresentação do anfitrião sobre si e minha hipótese é que quando esses valores estão preenchidos, a taxa de ocupação melhora devido a um aumento de confiança por parte do hóspede.

Vou então criar uma nova coluna para indicar apenas que essa descrição está preenchida, não entrando no mérito de avaliar o conteúdo preenchido.
"""

# criando uma coluna nova e convertando para booleano para indicar que a variável host_about tem valor preenchido
ds_treated[HAS_HOST_ABOUT] = ds_treated[HOST_ABOUT].notna()

"""E agora removo a coluna original."""

# removendo coluna original host_about
ds_treated.drop([HOST_ABOUT], axis=1, inplace= True)

ms.matrix(ds_treated)

"""##### Host_response_time, host_response_rate e host_acceptance_rate

Essas variáveis englobam indicadores de reação do anfitrião perante interações dos possíveis hóspedes.

Faço agora uma análise inicial.
"""

ds_treated[[HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE]].head(40)

"""Notem que as 3 variáveis podem conter valores nulos e que a variável host_response_time é uma variável categórica."""

ds_treated[[HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE]].info()

"""Vou optar por remover as linhas onde não há dado sobre tempo e taxa de resposta do anfitrião além de taxa de aceitação. Entendo que essas variáveis podem influenciar nas taxas de ocupação e prefiro remover do dataset as instâncias que não tem esse dado."""

# removendo instâncias com valores nulos para colunas host_responde_time, host_response_rate e host_acceptance_rate
colunas = [HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE]
ds_treated.dropna(subset=colunas, inplace=True)

"""Agora vou fazer os ajustes de tipagem e de conteúdo nessas colunas."""

# convertendo o tipo de host_response_time para o tipo category
ds_treated[HOST_RESPONSE_TIME] = ds_treated[HOST_RESPONSE_TIME].astype('category')

# Tratando as colunas host_response_rate e host_acceptance_rate
ds_treated[HOST_RESPONSE_RATE] = ds_treated[HOST_RESPONSE_RATE].str.replace('%', '').astype(int)
ds_treated[HOST_ACCEPTANCE_RATE] = ds_treated[HOST_ACCEPTANCE_RATE].str.replace('%', '').astype(int)

"""Conferindo o resultado dos ajustes"""

ds_treated[[HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE]].info()

ds_treated[[HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE]].head()

"""##### Host_is_superhost

Análise inicial na variável.
"""

# analisando primeiros valores
ds_treated[HOST_IS_SUPERHOST].head(10)

# verificando distribuição
ds_treated[HOST_IS_SUPERHOST].value_counts()

"""Vou tratar convertando para tipo booleano mapeando valor f e nulo para False e t para True."""

#convertendo para tipo bool
ds_treated[HOST_IS_SUPERHOST] = ds_treated[HOST_IS_SUPERHOST].map({'t': True, 'f': False}).fillna(False)

# verificando o resultado
ds_treated[HOST_IS_SUPERHOST].head(10)

"""##### Host_total_listings_count

Variável que informa a quantidade de acomodações sendo gerenciadas pelo anfitrião.
"""

# analisando a variável
ds_treated[HOST_TOTAL_LISTINGS_COUNT].info()

"""Nenhum ação necessária por hora por que não existem valores nulos e a tipagem está correta.

##### Host_identity_verified

Variável que informa sobre a verificação de identidade do anfitrião.
"""

# analisando a variável
ds_treated[HOST_IDENTITY_VERIFIED].info()

# analisando primeiros valores
ds_treated[HOST_IDENTITY_VERIFIED].head(10)

#convertendo para tipo bool
ds_treated[HOST_IDENTITY_VERIFIED] = ds_treated[HOST_IDENTITY_VERIFIED].map({'t': True, 'f': False})

# analisando o resultado
ds_treated[HOST_IDENTITY_VERIFIED].value_counts()

"""#### 4.3.5 Análise final da sessão"""

ds_treated.info()

"""Vou converter campos que podem ser do tipo category que ainda restaram."""

# convertendo para o tipo category
ds_treated[NEIGHBOURHOOD_CLEANSED] = ds_treated[NEIGHBOURHOOD_CLEANSED].astype('category')
ds_treated[ROOM_TYPE] = ds_treated[ROOM_TYPE].astype('category')
ds_treated[BATHS_CATEGORY] = ds_treated[BATHS_CATEGORY].astype('category')
ds_treated[HOST_PROXIMITY] = ds_treated[HOST_PROXIMITY].astype('category')

"""Mais uma visualização em forma de matriz"""

ms.matrix(ds_treated)

"""Vou fazer um último ajuste posicionando as colunas do dataset de acordo com as entidades com as quais estou trabalhando e também aproximando colunas correlatas."""

#colunas Acomodação
colunas_acomodacao = [LISTING_URL, ROOM_TYPE, ACCOMMODATES, BEDROOMS_ADPT, BEDS_ADPT, BATHS_INDEX, BATHS_CATEGORY, AMENITIES_COUNT, PRICE, MINIMUM_NIGHTS, MAXIMUM_NIGHTS, REVIEW_SCORES_RATING, NUMBER_OF_REVIEWS, NUMBER_OF_REVIEWS_LTM, NUMBER_OF_REVIEWS_L30D, LAST_REVIEW, REVIEWS_PER_MONTH, MONTHS_SINCE_REGISTERED, INSTANT_BOOKABLE, HOST_PROXIMITY]

#colunas Localização da Acomodação
colunas_localizacao = [NEIGHBOURHOOD_CLEANSED, HAS_NEIGHBOURHOOD_OVERVIEW]

#colunas Anfitrião
colunas_anfitriao = [HOST_SINCE, HOST_TOTAL_LISTINGS_COUNT, HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE, HOST_IS_SUPERHOST, HOST_IDENTITY_VERIFIED, HAS_HOST_ABOUT]

#definindo a ordem dos grupos
colunas_ordenadas = colunas_acomodacao + colunas_localizacao + colunas_anfitriao

#reposicionando as colunas
ds_treated = ds_treated[colunas_ordenadas]

ms.matrix(ds_treated)

"""## 5. Dicionário de dados

Incluo o dicionário de dados nesse trecho do notebook por que as variáveis presentes nesse momento no dataset serão profundamente trabalhadas nas próximas seções.

**Informações sobre os atributos:**
1. **listing_url** - URL da acomodação no site Airbnb.
2. **room_type** - Categoria da acomodação podendo ser 'Entire home/apt', 'Private room', 'Shared Room' ou 'Hotel room'.
3. **accommodates** - Quantidade máxima de hóspedes aceitos na acomodação. O limite inferior é 1 e o superior é 16.
4. **bedrooms_adpt** - Quantidade de quartos disponíveis na acomodação. Quando o tipo de acomodação for quarto compartilhado, esse campo recebe um valor fracionado inversamente proporcional a quantidade de hóspedes.
5. **beds_adpt** - Quantidade de camas disponíveis na acomodação.
6. **baths_index** - Valor numérico que representa a quantidade de banheiros disponíveis por hóspede.
7. **baths_category** - Categoria de banheiro, podendo ser 'dedicated', 'shared' ou 'no_bathroom'.
8. **amenities_count** - Quantidade de comodidades oferecidas na acomodação.
9. **price** - Valor numérico indicando o preço padrão em reais (R$) para a diária da acomodação. O preço pode ser diferente em períodos específicos mas aqui considerou-se apenas o valor padrão.
10. **minimum_nights** - Quantidade mínima de noites definida como padrão para a acomodação. O anfitrião pode configurar valores mínimos diferentes para períodos específicos mas nesse trabalho levamos em consideração apenas o valor padrão.
11. **maximum_nights** - Quantidade máxima de noites definida como padrão para a acomodação. O anfitrião pode configurar valores máximos diferentes para períodos específicos mas nesse trabalho levamos em consideração apenas o valor padrão.
12. **review_scores_rating** - Valor numérico que indica a pontuação da avaliação feita pelo hóspedes para a acomodação. Pode variar entre 0 e 5.
13. **number_of_reviews** - Quantidade total de avaliações feitas por hóspedes ao longo da vida da acomodação na plataforma.
14. **number_of_reviews_ltm** - Quantidade total de avaliações da acomodação feitas por hóspedes ao longo dos últimos 12 meses (contanto 12 meses antes do dia 22/09/2023, que foi a data da geração do dataset).
15. **number_of_reviews_l30d** - Quantidade total de avaliações feitas por hóspedes nos últimos 30 dias (referente a 22/09/2023 também).
16. **last_review** - Data da última avaliação feita por um hóspede para a acomodação.
17. **reviews_per_month** - Quantidade média de avaliações feitas por hóspedes por mês, para a acomodação.
18. **months_since_registered** - Quantidade de meses desde que a acomodação foi registrada na plataforma Airbnb.
19. **instant_bookable** - Valor booleano que indica se a acomodação aceita reservas automaticamente, sem necessidade de aceite por parte do anfitrião.
20. **host_proximity** - Categoria que indica se o anfitrião está localizado próximo da acomodação, podendo ter os seguintes valores: 'Very Close', 'Relatively Close', 'Far' ou 'No Information'.
21. **neighbourhood_cleansed** - Categoria que indica o bairro onde se localiza a acomodação.
22. **has_neighbourhood_overview** - Valor booleano que indica se há uma descrição do bairro na publicação da acomodação.
23. **host_since** - Data de registro do anfitrião na plataforma Airbnb. Pode ser a data em que a pessoa entrou como anfitrião ou como hóspede, o evento que aconteceu primeiro, em que ela teve que se registrar.
24. **host_total_listings_count** - Quantidade total de acomodações sob gestão do anfitrião.
25. **host_response_time** - Categoria que indica o tempo de resposta padrão do anfitrião para aquela acomodação, podendo receber os valores 'within an hour', 'within a few hours', 'within a day' ou 'a few days or more'.
26. **host_response_rate** - Valor numérico que indica a taxa de resposta do anfitrião para a acomodação.
27. **host_acceptance_rate** - Valor numérico que indica a taxa de aceite do anfitrião para os pedidos de reserva da acomodação.
28. **host_is_superhost** - Valor booleano que indica se o anfitrião é um super host. Esse selo é dado pela própria plataforma Airbnb em avaliações feitas a cada 3 meses. Para mais detalhes, veja as [regras](https://www.airbnb.com.br/help/article/829).
29. **host_identity_verified** - Valor booleano que indica se o anfitrião deve a identidade verificada.
30. **has_host_about** - Valor booleano que indica se a descrição sobre o anfitrião está preenchida.

## 6. Análise visual dos dados

### 6.1 Análise unidimensional

Nesta seção farei análises de cada variável de maneira isolada, sem relacioná-las com outras variáveis.

#### 6.1.1 Entidade Acomodação

Primeiramente vou usar histogramas para visualizar a distribuição das variáveis quantitativas.
"""

# Histogramas dos dados de Acomodação
ds_treated[colunas_acomodacao].hist(figsize = (20,12), color='skyblue', edgecolor='black')
plt.show()

"""Alguns comentários sobre os resultados:

*   a maioria dos gráficos apresente uma distribuição de dados com assimetria à direita;
*   alguns gráficos como os das variáveis review_scores_rating e last_review apresentam assimetria à esquerda;
*   o gráfico da variável maximum_nights apresenta um padrão multimodal; a interpretação disso indica que alguns anfitriões de fato usam Airbnb para aluguel por temporada em períodos curtos (o grupo presente no primeiro pico) mas outros usam a plataforma para aluguéis de mais longo prazo.
*   o gráfico da variável accommodates apresenta um padrão bimodal com assimetria à direita;
*   o único gráfico que se aproxima de uma distribuição normal é o da variável amenities_count, mas tem uma leve assimetria à direita.
  .

Vou analisar com mais detalhe a variável accommodates, ainda usando histograma, para identificar em que valores ficam os dois picos do bimodal.
"""

# exibir histograma apenas de accommodates
render_histogram_chart(ds_treated[ACCOMMODATES], 30)

"""Podemos agora notar claramente que para essa variável os dois picos acontecem com os valores 2 e 4. Interessante notar também um padrão normal com assimetria à direita quando observamos valores pares e outro padrão normal quando observamos valores ímpares.

Vou destarcar esse comportamento diferenciando por cores.
"""

# Separar os dados em pares e ímpares
dados_pares = ds_treated[ACCOMMODATES][ds_treated[ACCOMMODATES] % 2 == 0]
dados_impares = ds_treated[ACCOMMODATES][ds_treated[ACCOMMODATES] % 2 != 0]

# Configurar o número desejado de bins
numero_de_bins = 30

# Plotar as barras para valores pares em azul
plt.hist(dados_pares, bins=numero_de_bins, color='blue', alpha=0.7, label='Even')

# Plotar as barras para valores ímpares em amarelo
plt.hist(dados_impares, bins=numero_de_bins, color='yellow', alpha=0.7, label='Odd')

plt.title('Histogram')
plt.xlabel(ds_treated[ACCOMMODATES].name)
plt.legend()
plt.show()

"""Agora vou usar boxplot para visualizar quartis, mediana e a presença de outliers."""

# Boxplot
ds_treated[colunas_acomodacao].plot(kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False, figsize = (20,12))
plt.show()

"""Para algumas variáveis, a forte concentração de valores em determinada faixa junto com a presença de outliers faz com que a visualização da distribuição de dados por boxplot fique dificultada. A variável price é o exemplo mais extremo desse cenário.

Vou olhar em mais detalhe essa variável.
"""

# Boxplot
ds_treated[PRICE].plot(kind = 'box')
plt.show()

"""Notem que gerar um gráfico isolado dessa variável não ajuda, mesmo com esse gráfico ficando maior.

Vou aplicar uma função logarítmica para tornar a distribuição mais fácil de visualizar.
"""

# visualizando usando função logaritmica
plt.boxplot(np.log(ds_treated[PRICE]))
plt.show()

"""A visualização fica melhor mais ainda assim, existem dificuldades pela grande presença de outliers e pelo fato do eixo y receber valores não facilmente interpretáveis.

Vou gerar o gráfico sem a presença de outliers.
"""

# boxplot de price
render_one_boxplot(ds_treated[PRICE], False)

"""Essa versão do gráfico, sem os outliers, é a que mais facilita a leitura. Além disso incluí na mesma os valores do primeiro e terceiro quartis e também a mediana.

Outra variável bem difícil de ler por boxplot usando as configurações padrão é a minimum_nights.

Vejamos como fica a visualização dela retirando os outliers.
"""

render_one_boxplot(ds_treated[MINIMUM_NIGHTS], False)

"""Notem que existe uma concentração tão grande no valor 2 que o gráfico exibe o primeiro quartil e a mediana com valor 2, sobrepostos.

Posso revisitar a estatística dessa variável com o método describe para confirmar a interpretação.
"""

ds_treated[MINIMUM_NIGHTS].describe()

#gráfico de densidade de minimum_nights
render_density_chart(ds_treated[MINIMUM_NIGHTS])

"""A curva de densidade comprova a observação.

Preciso analisar com mais detalhe a variável number_of_reviews_ltm por que ela representa a variável alvo com a qual quero trabalhar. Lembrando que essa variável informa a quantidade de reviews nos últimos 12 meses. Já vimos na primeira renderização de histogramas que ela tem uma assimetria à direita e também vimos na renderização boxplot que existe uma certa quantidade de outliers.

Faço agora uma renderização excluindo outliers.
"""

render_one_boxplot(ds_treated[NUMBER_OF_REVIEWS_LTM], False)

"""Além das variáveis numéricas, a entidade Acomodação também engloba variáveis de outras tipagens. São as variáveis room_type, baths_category, last_review, instant_bookable e host_proximity. Vejamos agora dados sobre elas.

Para  variáveis categóricas com poucas categorias, utilizarei sempre um gráfico de barras ao lado de um gráfico do tipo donut, para vermos valores absolutos e valores percentuais com duas visões distintas.
"""

#renderizar gráficos bar e donut para room_type
render_bar_and_donut_charts(ds_treated[ROOM_TYPE])

"""A maioria das acomodações são oferecidas na modalidade propriedade completa."""

#renderizar gráficos bar e donut para baths category
render_bar_and_donut_charts(ds_treated[BATHS_CATEGORY])

"""A maioria das acomodações tem um banheiro dedicado."""

#renderizar gráficos bar e donut para instant_bookable
render_bar_and_donut_charts(ds_treated[INSTANT_BOOKABLE])

"""Interessante perceber que a maioria dos anfitriões prefere revisar os pedidos de hospedagem antes de aceitar."""

#renderizar gráficos bar e donut para host_proximity
render_bar_and_donut_charts(ds_treated[HOST_PROXIMITY])

"""Agora exibo a variável last_review, do tipo datetime, usando uma gráfico de curva de densidade. Como essa variável informa a data da última revisão recebida pela acomodação, poderemos ter uma noção do nível de atividade das diversas acomodações segundo esse critério."""

# renderizando a curva de densidade da variável last_review
render_density_chart(ds_treated[LAST_REVIEW])

"""Podemos notar uma concentração alta de últimas revisões acontecendo em períodos recentes. Por essa observação, podemos afirmar que as acomodações presentes na listagem tiveram pelo menos uma avaliação em períodos recentes.

#### 6.1.2 Entidade Localização da Acomodação

Essa entidade engloba apenas as variáveis neighbourhood_cleansed e has_neighbourhood_overview. Vejamos como elas se comportam.

Vou iniciar com a variável neighbourhood_cleansed que indica o bairro onde se localiza a acomodação. Notem que essa é uma variável categórica com um grande número de categorias. Vou optar por um gráfico de barras na horizontal para facilitar a leitura.
"""

# renderizar gráfico de barras horizontal dos bairros
render_bar_chart_horizontal(ds_treated[NEIGHBOURHOOD_CLEANSED])

"""Podemos ver que existe uma grande quantidade de acomodações disponíveis em poucos bairros com outros tendo pouca oferta. Quero agora conferir esses dados calculando o percentual acumulativo, para ter uma visão 80/20."""

# analisando percentuais de maneira acumulativa
print_cumulative_percentage(ds_treated[NEIGHBOURHOOD_CLEANSED])

"""Notem que os 9 primeiros bairros presentes na lista contemplam 80% das acomodações disponíveis no Rio de Janeiro. O bairro de Copacabana concentra mais de 1/3 das acomodações da cidade.

Sigo agora para a segunda variável dessa entidade, has_neighbourhood_overview, que é aquele que indica se o anfitrião colocou uma descrição do bairro onde se localiza a acomodação.
"""

render_bar_and_donut_charts(ds_treated[HAS_NEIGHBOURHOOD_OVERVIEW])

"""Cerca de 2/3 das acomodações receberam essas descrições do bairro por parte de seus anfitriões.

#### 6.1.3 Entidade Anfitrião

As variáveis englobadas pela entidade Anfitrião são host_since, host_total_listings_count, host_response_time, host_response_rate, host_acceptance_rate, host_is_superhost, host_identity_verified e has_host_about. Temos data, categorias, valores numéricos e booleanos.


Existe uma observação importante a ser feita aqui, para que melhor interpretemos os dados. O dataset deste trabalho é um dataset [desnormalizado](https://en.wikipedia.org/wiki/Denormalization) quando olhamos para ele sob a ótica da entidade Anfitrião. Existem anfitriões que administram mais de uma acomodação então dados referentes a esses anfitriões estarão repetidos no dataset. Se não ficarmos atentos a esse detalhe, podemos interpretar de maneira errada alguns dos resultados.

Logo abaixo eu apresento um recorte do dataset mostrando dados referentes ao anfitrião que administra a maior quantidade de acomodações da cidade. Notem como os dados desse anfitrião se repetem em cada acomodação, comprovando a desnormalização.
"""

# obtendo as primeiras acomodações do anfitriao que administra a maior quantidade de acomodacoes
ds[ds[HOST_TOTAL_LISTINGS_COUNT] == ds[HOST_TOTAL_LISTINGS_COUNT].max()][[HOST_NAME, HOST_RESPONSE_TIME, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE, HOST_SINCE, HOST_LISTINGS_COUNT, HOST_TOTAL_LISTINGS_COUNT]].head(30)

"""Vejamos o comportamento dessas variáveis.

Primeiramente vou usar histogramas para visualizar a distribuição das variáveis quantitativas.
"""

# Histogramas dos dados de Acomodação
ds_treated[colunas_anfitriao].hist(figsize = (10,6))
plt.show()

"""Podemos notar um padrão normal para variável host_since, uma assimetria à direita para host_total_listings_count, e assimetrias à esquerda para host_response_rate e host_acceptance_rate.

Agora vou usar boxplot para visualizar quartis, mediana e a presença de outliers.
"""

# Boxplot
ds_treated[colunas_anfitriao].plot(kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False, figsize = (20,12))
plt.show()

"""As duas primeiras variáveis apresentam alta concentração dos dados em determinadas faixas além da presença de outliers o que dificulta a interpretação do boxplot.

Vou gerar novamente os gráficos desconsiderando outliers.
"""

render_one_boxplot(ds_treated[HOST_TOTAL_LISTINGS_COUNT], False)

render_one_boxplot(ds_treated[HOST_RESPONSE_RATE], False)

"""Notem que agora a leitura fica bem mais fácil.

Observo agora a variável host_since usando gráfico de densidade, assim como fiz com outra variável de data na seção anterior.
"""

# renderizar gráfico de densidade da variável host_since
render_density_chart(ds_treated[HOST_SINCE])

"""Ficam evidentes os dois picos acontecendo em 2014 e 2016, anos em que a cidade recebeu os grandes eventos Copa do Mundo de futebol masculino e Olimpíadas.

Notem que os valores presentes nessa variável não são datas de registro das acomodações e sim de registro dos anfitriões administradores das acomodações na plataforma Airbnb. Se lembrarmos que esse é um dataset desnormalizado, como comentei no início da sessão, devemos considerar que esses picos do gráfico podem estar incrementados pela existência de múltiplas acomodações sendo administradas por um mesmo anfitrião.

Quero observar com mais detalhe a variável host_total_listings_count. O histograma tem uma assimetria à direita mas notem que existe ocorrências no lado direito do diagrama.

Lembrando que essa variával indica a quantidade de acomodações sob administração do anfitrião administrador da acomodação em questão então a característica de desnormalização do dataset também afeta essa leitura.

Para entendermos o comportamento desse histograma nos seus extremos do eixo X, podemos analisar os valores usando head e tail, ordenando pelo índice do retorno de value_counts.
"""

# verificando a contagem de anfitriões que administram poucas acomodações
ds_treated[HOST_TOTAL_LISTINGS_COUNT].value_counts().sort_index().head(20)

# verificando a contagem de anfitriões que administram muitas acomodações
ds_treated[HOST_TOTAL_LISTINGS_COUNT].value_counts().sort_index().tail(20)

"""Num primeiro momento podemos considerar que há uma inconsistência no dado visto que a variável indica um valor absoluto que tem semântica de contagem mas esse valor é diferente da contagem feita pelo nosso próprio código.

O valor 1803, máximo dessa variável, indica que há um anfitrião que administra 1803 acomodações. Sabendo que esse valor se repete para cada acomodação (por causa da desnormalização), deveríamos imaginar que a nossa contagem também deveria chegar ao número 1803, correto? No entanto, precisamos lembrar que um anfitrião pode administrar acomodações em diferentes cidades e nosso dataset considera apenas a cidade do Rio de Janeiro. Além disso, já fizemos ações de limpeza no dataset o que pode ter eliminado da listagem algumas acomodações do anfitrião.

Outra consideração importante é que pode acontecer, principalmente com valores mais baixos, de dois anfitriões administrarem a mesma quantidade de acomodações. Nesse caso, a contagem de ocorrências desses dois serão somadas na agregação do histograma.

Mesmo com essa aparente inconsistência na variável, ela segue sendo importante para o meu trabalho visto que minha hipótese é que administrar muitas acomodações pode influenciar na taxa de ocupação.

Uma maneira de tentar visualizar os dados diminuindo o impacto da agregação feita pelos histogramas seria usar um gráfico de linhas.

Vejamos como fica com esse método.
"""

# renderizando quantidade total de acomodações do anfitrião em gráfico de linhas
render_value_counts_as_line(ds_treated[HOST_TOTAL_LISTINGS_COUNT])

"""O gráfico de linhas já apresenta o eixo y numa ordem de grandeza mais adequada para os valores da variável, mas ainda assim, a interpretação que podemos fazer a partir dele não é a melhor.

Vejamos como fica um gráfico de dispersão para a mesma variável:
"""

# renderiza a quantidade de acomodações do anfitrião numa visão em pontos usando scatter plots
render_value_counts_as_dots(ds_treated[HOST_TOTAL_LISTINGS_COUNT])

"""Agora sim podemos ter uma percepção melhor do que está acontecendo com essa variável, condizente com os retornos dos métodos head e tail executados mais acima.

Vejamos agora as variáveis restantes, de tipos categórico e booleano. São elas host_response_time, host_is_superhost, host_identity_verified e has_host_about.
"""

# renderizando gráfico de barras e donut para variável host_response_time
render_bar_and_donut_charts(ds_treated[HOST_RESPONSE_TIME])

"""Podemos notar que quase 62% das acomodações indicam que seus hosts respondem em até 1 hora e outros 22% respondem em algumas horas. O restante demora mais para responder."""

# renderizando gráfico de barras e donut para variável host_is_superhost
render_bar_and_donut_charts(ds_treated[HOST_IS_SUPERHOST])

"""Esses gráficos mostram que 41% das acomodações indicam que tem hosts avaliados como super hosts. Exitem [regras específicas](https://www.airbnb.com.br/help/article/828) que precisam ser obedecidas para que um anfitrião seja avaliado dessa maneira."""

# renderizando gráfico de barras e donut para variável host_identity_verified
render_bar_and_donut_charts(ds_treated[HOST_IDENTITY_VERIFIED])

"""A grande maioria dos anfitriões tem suas identidades verificas. Esse tipo de funcionalidade tem ficado bem comum nos diversos produtos digitais e serve como uma abordagem antifraude. Minha hipótese é que acomodações cujos anfitriões tem identidade verificada performam melhor nas suas taxas de ocupação."""

# renderizando gráfico de barras e donut para variável has_host_about
render_bar_and_donut_charts(ds_treated[HAS_HOST_ABOUT])

"""Exatamente 58% das acomodações apresentam descrições sobre seus hosts.

### 6.2 Análise multidimensional

Nesta seção farei análises das variáveis relacionando-as umas com as outras.

#### 6.2.1 Matriz de correlação

Inicio gerando uma matriz de correlação, para observarmos a correlação entre as variáveis numéricas.
"""

# Matriz de Correlação com Seaborn
plt.figure(figsize = (12,10))
sns.heatmap(ds_treated.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=.5, fmt=".2f", annot_kws={"size": 8});

"""É interessante observar os valores mais altos mas também as manchas presentes na matriz.

No canto superior esquerdo temos uma mancha relacionando variáveis que indicam características físicas das acomodações e faz todo sentido existir uma correlação entre essas características. A quantidade de hóspedes aceitos, a quantidade quartos e a quantidade de camas tem necessariamente que ter uma relação linear positiva. A variável baths_index tem uma relação negativa com a quantidade de hóspedes por que usei justamente esse número como denominador para calcular o índice. Quanto mais hóspedes, mais afetado negativamente fica o índice.

No centro da matriz existe outra mancha indicando correlação positiva alta entre as variáveis mas todas elas tem relação com a quantidade de reviews então faz todo sentido isso acontecer.

As manchas mais interessantes são aquelas localizadas distante da diagonal, pois sinalizam uma correlação entre variáveis que a princípio não tem relação umas com as outras. Vejamos por exemplo as manchas presentes na linha correspondente à variável amenities_count. Existe uma leve relação linear positiva com números relacionados aos reviews o que indica que quanto mais itens de conveniência, melhor são as pontuações de review e a quantidade destes (meu proxy para taxa de ocupação). Também aparece uma leve relação linear positiva com valores de taxa de resposta e taxa de aceites do anfitrião, assim como ao fato do anfitrião ser Super Host. Eu interpreto essa correlação como uma maior habilidade do anfitrião em configurar corretamente as acomodações sob sua administração.

Outra mancha que vale destacar é aquela que relaciona variáveis de review com variáveis de comportamento do anfitrião. Notem que existe uma relação linear positiva entre esses números, o que faz sentido visto que anfitriões com boas taxas de tempo de resposta e de aceite naturalmente tendem a levar suas acomodações a terem mais ocupação e mais reviews.

Notem também uma mancha azul na lateral esquerda, no meio da figura. Apesar dos valores serem baixos, parece acontecer uma relação linear negativa entre o grupo de dados de quantidade de reviews e o grupo de dados que indicam o tamanho total da acomodação (hóspedes, quartos, camas e banheiros). Ou seja, quanto maior a acomodação, menor seriam as quantidades de reviews. Essa relação negativa parece fazer sentido pois é bem mais fácil manter ocupada com hóspedes uma acomodação menor do que uma maior, para grupos grandes.

#### 6.2.2 Boxplots com pequenos múltiplos

Como a variável alvo é a quantidade de reviews nos últimos 12 meses, vale a pena entendermos a relação dessa variável em específico com diversas outras.

Inicio fazendo uma análise de pequenos múltiplos com boxplot passando pelas variáveis categóricas do dataset.

Primeiramente uma visualização considerando outliers.
"""

# renderizar boxplots em pequenos múltiplos considerando outliers
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM, ROOM_TYPE, True)

"""Notem a amplitude maior de valor para as acomodações do tipo 'Entire home/apt' e 'Private room'.

E agora uma visualização usando o mesmo gráfico mas sem considerar outliers.
"""

# renderizar boxplots em pequenos múltiplos não considerando outliers
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM, ROOM_TYPE, False)

"""A mediana maior para a variável de quantidade de reviews é claramente das acomodações do tipo 'Entire home/apt'.

Vejamos agora se existe alguma diferença de comportamento dessa variável de acordo com a categoria de banheiro.
"""

# renderizar boxplots em pequenos múltiplos não considerando outliers para categorias de banheiros
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM, BATHS_CATEGORY, False)

"""Sim, existe. Acomodações com banheiro do tipo 'dedicated' tem mediana maior mas provavelmente essa tendência decorre do fato de que banheiros dedicados existem mais comummente em acomodações do tipo 'Entire home/apt' e já vimos que esse tipo de acomodação tem mediana maior para quantidade de reviews.

Vejamos agora se as acomodações com agendamento automático ganham daquelas sem essa característica.
"""

# renderizar boxplots em pequenos múltiplos não considerando outliers para instant_bookable
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM, INSTANT_BOOKABLE, True)

"""Sim, podemos ver pelos dados estatísticos do bloxplot que acomodações com agendamento automático no geral ganham em quantidade de reviews.

Vejamos agora considerando a proximidade do anfitrião em relação à acomodação administrada.
"""

# renderizar boxplots em pequenos múltiplos considerando outliers para host_proximity
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM,  HOST_PROXIMITY, True)

# renderizar boxplots em pequenos múltiplos não considerando outliers para host_proximity
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM,  'host_proximity', False)

"""Podemos notar claramente que esse dado não influi diretamente na quantidade de reviews.

Vejamos agora se o fato de existir uma descrição do bairro influencia na quantidade de reviews.
"""

# renderizar boxplots em pequenos múltiplos considerando outliers para has_neighbourhood_overview
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM, HAS_NEIGHBOURHOOD_OVERVIEW, True)

# renderizar boxplots em pequenos múltiplos não considerando outliers para has_neighbourhood_overview
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM, HAS_NEIGHBOURHOOD_OVERVIEW, False)

"""Podemos notar uma pequena diferença para melhor na quantidade de reviews daquelas acomodações com descrição do bairro. Notem que isso pode ser um impacto direto dessa característica ou pode estar vindo de outra característica da acomodação de maneira indireta.

Vejamos agora como a variável de comporta de acordo com o tempo de resposta do anfitrião.
"""

# renderizar boxplots em pequenos múltiplos considerando outliers para host_response_time
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM,  HOST_RESPONSE_TIME, True)

"""Podemos ver claramente que anfitriões com melhores tempos de resposta, ou seja, tempos de resposta mais baixos, levam a uma quantidade de reviews maior das acomodações.

Vejamos se Superhosts geram números melhores para essa variável.
"""

# renderizar boxplots em pequenos múltiplos considerando outliers para host_is_superhost
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM,  HOST_IS_SUPERHOST, True)

"""O anfitrião ganha o badget de superhost justamente quando consegue ter métricas melhores em diversos fatores. Era natural que nessa comparação o desempenho destes fossem melhores.

Vejamos agora a relação com a variável de identidade verificada.
"""

# renderizar boxplots em pequenos múltiplos considerando outliers para host_identity_verified
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM,  HOST_IDENTITY_VERIFIED, True)

"""O resultado também indica métricas melhores para quem tem identidade verificada mas esse é outro caso onde o diferencial pode estar vindo para essa variável de maneira indireta.

Vejamos agora se acomodações que tem uma descrição do anfitrião tem desempenho melhor.
"""

# renderizar boxplots em pequenos múltiplos considerando outliers para has_host_about
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM,  HAS_HOST_ABOUT, True)

# renderizar boxplots em pequenos múltiplos não considerando outliers para has_host_about
render_box_plot_as_small_multiples(ds_treated, NUMBER_OF_REVIEWS_LTM,  HAS_HOST_ABOUT, False)

"""Ao observar os dois gráficos podemos dizer que o comportamento da variável é quase igual para ambos os casos.

Deixei a categoria de bairro por último por ser uma variável categórica com muitas opções diferentes. Como eu quero exibir os nomes dos bairros para uma melhor leitura, já vou iniciar optando por uma impressão de múltiplos boxplots na horizontal.
"""

# renderiza um gráfico de boxplots na horizontal
# Informa os bairros e ordena pelo valor de mediana da métrica de quantidade de reviews dos últimos 12 meses
# limitando aos bairros que tenham pelo menos 100 acomodações
render_multiple_horizontal_box_plots(ds_treated, NEIGHBOURHOOD_CLEANSED, NUMBER_OF_REVIEWS_LTM, 100)

"""Notem que Centro, Copacabana e Ipanema são os bairros com valores de mediana maiores, para quantidade de avaliações nos últimos 12 meses. Na análise visual, parece que alguns bairros tem valores de mediana iguais ou bem similares. Vou conferir esses valores para os primeiros bairros da figura."""

#valores de medianas para bairros selecionados
ds_treated[ds_treated[NEIGHBOURHOOD_CLEANSED].isin(['Centro','Copacabana','Ipanema', 'Glória', 'Leblon'])].groupby(NEIGHBOURHOOD_CLEANSED)[NUMBER_OF_REVIEWS_LTM].median().dropna()

"""#### 6.2.3 Análises visuais de 3 variáveis

##### 6.2.3.1 Duas variáveis numéricas e uma categórica

Vou analisar a relação entre os valores numéricos de quantidade de quartos e quantidade de camas e usar a variável categórica room_type para diferenciar os pontos por cores. Lembrando que esses valores numéricos foram ajustados nas seções anteriores de modo a não termos valores nulos e também que podermos ter valores que são frações de 1, para quartos.
"""

# Scatter Plot com Seaborn
sns.pairplot(ds_treated[[BEDROOMS_ADPT, BEDS_ADPT, ROOM_TYPE]], hue = ROOM_TYPE, height = 3.5);

"""Observações por categoria:

*   Shared room - valores numéricos para quantidade de quartos dessa categoria variam entre 0 e 1 (índice calculado) e podemos notar que isso faz com que todos os pontos dessa categoria fiquem bem próximos do eixo, se distanciando do centro conforme mais camas estejam presentes no quarto.
*   Private room - nesse caso não existe fracionamento e portanto, não existem valores numéricos menores do que 1, por que não é um cenário de compartilhamento de quarto. Mas notem que existem tanto pontos distribuídos ao longo do eixo de camas, o que indica cenários de poucos quartos e muitas camas, como pontos distribuídos ao longo do eixo de quartos, o que indica um cenário de vários quartos privados com, talvez, uma cama em cada quarto.
*   Hotel room - são poucas ocorrências de quartos de hotel corroborando dados que vimos em seções anteriores, com cenários específicos de combinações de quantidades de quartos e camas.
*   Entire home/apt - essa categoria é aquela com mais ocorrências e a que aparece com maior distribuição ao longo dos eixos. Como ela é uma categoria que atende diferentes tipos de dimensão de acomodações, é natural que ela suporte melhor as diferentes combinações de quantidade de quartos e camas e que isso fique bem representado na nossa figura.

Depois de observar esse gráfico, podemos supor que existe uma relação linear maior entre as variáveis de quantidade quartos e quantidade de camas na categoria de acomodação inteira, em comparação com as outras categorias. Vou conferir essa hipótese:
"""

# Calcula a correlação linear entre as duas colunas
correlacao_linear = ds_treated[BEDROOMS_ADPT].corr(ds_treated[BEDS_ADPT])
print(f"Correlação Linear geral: {correlacao_linear}\n")

# Calcula a correlação para cada categoria usando groupby
correlacoes_por_categoria = ds_treated.groupby(ROOM_TYPE).apply(lambda group: group[BEDROOMS_ADPT].corr(group[BEDS_ADPT]))
print(f"Correlação Linear por categoria: {correlacoes_por_categoria}")

"""Notem que de fato a correlação é mais alta para as categorias 'Entire home/apt' e 'Hotel room' e menor para 'Private room'. Para a categoria 'Share room' a relação é até negativa.

##### 6.2.3.2 Duas variáveis categóricas e uma numérica

Vou agora usar gráficos do tipo **heatmap** para analisar valores de algumas **variáveis numéricas** segundo o cruzamento de **duas variáveis categóricas**: **bairro** e **tipo de acomodação**.

Para melhor visualização dos heatmaps, estudei o tema de paleta de cores e vi que existem modelos diferentes que devem ser usados em situações específicas: divergente, qualitativa e sequencial. Esse site é uma boa referência: https://www.codecademy.com/article/seaborn-design-ii

Coloquei uma imagem com uma coleção de paletas de referência na seção de anexos, no final do notebook.

Vou usar esses conceitos nos gráficos abaixo.

Nessa primeira análise, eu gostaria de verificar se algum bairro em específico chama a atenção por ter uma grande quantidade de um tipo específico de acomodação.

Usei o padrão de cores RdBu_r, de cores contrastantes, para destacar valores altos e baixos. Apesar da contagem de ocorrências ser naturalmente uma métrica sequencial, eu adotei uma palete de tipo divergente, e inverti a paleta (o _r no nome da paleta faz a inversão). Fiz isso por que a maioria das células do heatmap tem valores baixos de contagem o que padronizou a figura com a cor azul mais escuro, do extremo do espectro. Isso permite uma visualização mais clara das ocorrências fora desse padrão baixo de contagem.
"""

# cruzando as duas variáveis categóricas e fazendo a contagem de ocorrências com
# o método crosstab
tabela_contingencia = pd.crosstab(ds_treated[NEIGHBOURHOOD_CLEANSED], ds_treated[ROOM_TYPE])

# Ajustando o tamanho da figura
plt.figure(figsize=(10, 25))

# Criando o heatmap com seaborn sem imprimir valores, para focar apenas no visual
sns.heatmap(tabela_contingencia, annot=False, cmap="RdBu_r", fmt='d')

# Exibindo o gráfico
plt.show()

"""Notem que esse gráfico ajuda a termos uma percepção visual do volume de acomodações segundo as duas categorias escolhidas. E na análise visual, parece que o bairro da Tijuca tem mais acomodações do tipo 'Private room' do que do tipo 'Entire home/apt'. Vou conferir:"""

tabela_contingencia[tabela_contingencia.index == 'Tijuca']

"""De fato, a Tijuca tem mais acomodações do tipo 'Private room' do que do tipo 'Entire home/apt', fugindo do padrão dos outros bairros da cidade.

Vou agora montar o heatmap considerando a variável numérica **preço**. Vejamos como ela se comporta segundo esses critérios. Vou usar a paleta de cores Purples, do tipo sequencial. Dessa vez vou imprimir os valores.
"""

# Criando uma tabela de pivot com a mediana dos preços
tabela_mediana_preco = ds_treated.pivot_table(values=PRICE, index=NEIGHBOURHOOD_CLEANSED, columns=ROOM_TYPE, aggfunc='median')

# Ajustando o tamanho da figura
plt.figure(figsize=(10, 25))

# Criando o heatmap com seaborn
# usando paleta Purples pelo fato de preço ser uma variável sequencial
sns.heatmap(tabela_mediana_preco, annot=True, cmap="Purples", fmt='.0f')

# Exibindo o gráfico
plt.show()

"""Algumas observações interessantes a partir desse heatmap:


*   a primeira delas e talvez a mais importante é que a baixa quantidade de acomodações disponíveis em alguns bairros e de tipos específicos, faz com que apareçam valores inesperados em alguns casos. Isso precisa ser levado em conta para uma correta interpretação dos valores;
*   chama a atenção a ocorrência de valores maiores de preço em alguns bairros para 'Hotel room' do que para 'Entire home/apt';
*   chamam a atenção os valores mais altos de mediana para acomodação do tipo 'Entire room/apt' dos bairros São Conrado, Joá e Rocinha. Principalmente esse último bairro por se tratar de uma comunidade carente.

Farei uma análise em específico para esse caso:

"""

# ocorrências para o bairro Rocinha
ds_treated[ds_treated[NEIGHBOURHOOD_CLEANSED] == 'Rocinha']

"""Existem 3 ocorrências no bairro da Rocinha do tipo 'Entire room/apt'. Ao acessar a url dessas 3, apenas uma delas se declara como estando na Rocinha. As outras duas estão localizadas do lado da Gávea, no alto da montanha. Como a informação de bairros foi calculada a partir dos dados de latitude e longitude, a informação do bairro como Rocinha veio desse processamento. É importante destacarmos esse acontecimento por que muito provavelmente pode estar acontecendo para outros bairros também, para acomodações localizadas na fronteira entre bairros distintos. Esse caso ficou mais evidente por que existem poucas ocorrências de acomodações na Rocinha e por que há um grande nível de desigualdade sócio-econômica entre os dois bairros, característica marcante da cidade do Rio de Janeiro.

Agora farei uma análise das pontuações obtidas pelas acomodações (review score), usando o mesmo modelo de heatmap. Dessa vez, optei por montar uma paleta de cores própria, para seguir uma organização visual e de métricas usada comumente em avaliações do tipo [CSAT](https://en.wikipedia.org/wiki/Customer_satisfaction), de satisfação do consumidor.

A pontuação pode variar entre 0 e 5 então organizei esse espectro de valores em intervalos de 0.25 ficando com 20 intervalos.

Para os 10 primeiros intervalos, que vão de 0 a 2.5, usei uma escala de cores em vermelho. Para os intervalos entre 2.5 e 3.5 eu usei uma escala de amarelos. Para os intervalos entre 3.5 e 4.5 eu usei uma escala de verde claro e finalmente, para o intervalo entre 4.5 e 5.0, uma escala de verde escuro.

Essa abordagem valoriza visualmente as pontuações mais altas, no extremo superior do espectro de valores numéricos.
"""

# Criando uma tabela de pivot com a mediana dos preços
tabela_mediana = ds_treated.pivot_table(values=REVIEW_SCORES_RATING, index=NEIGHBOURHOOD_CLEANSED, columns=ROOM_TYPE, aggfunc='median')

# Ajustando o tamanho da figura
plt.figure(figsize=(10, 25))

paleta_cores = get_color_pallete_as_csat()

# Criando o heatmap com seaborn

#paleta de cores divergente
#sns.heatmap(tabela_mediana, annot=True, cmap="RdYlGn", fmt='.2f')
# paleta de cores sequencial
#sns.heatmap(tabela_mediana, annot=True, cmap="YlGn", fmt='.2f')
# paleta de cores csat
sns.heatmap(tabela_mediana, annot=True, cmap=paleta_cores, fmt='.2f')

# Exibindo o gráfico
plt.show()

"""Algumas observações referentes ao heatmap de pontuações:

*   De maneira geral, as pontuações são do patamar mais alto. Podemos ter essa percepção pelo fato das cores verde escuro serem as predominantes.
*   O padrão de cores também ajuda a diferenciar os bairros onde os diferentes tipo de acomodações estão no mesmo patamar de notas daqueles bairros onde há diferença nesses patamares.
*   O baixo volume de acomodações de um determinado tipo em alguns bairros também pode influenciar para o aparecimento de pontuações muito baixas ou muito altas no diagrama, chamando a atenção.

Para finalizar as análises visuais, vou montar um heatmap com a quantidade de reviews dos últimos 12 meses (minha variável alvo), organizando mais uma vez por bairro e tipo de acomodação. Usarei uma paleta de cores do tipo sequencial, de 3 cores.
"""

# Criando uma tabela de pivot com a mediana da quantidade de reviews
tabela_mediana = ds_treated.pivot_table(values=NUMBER_OF_REVIEWS_LTM, index=NEIGHBOURHOOD_CLEANSED, columns=ROOM_TYPE, aggfunc='median')

# Ajustando o tamanho da figura
plt.figure(figsize=(10, 25))

# Criando o heatmap com seaborn
# usando paleta de cores do tipo sequencial já que quantidade de reviews tem essa característica
sns.heatmap(tabela_mediana, annot=True, cmap="PuBuGn", fmt='.1f')

# Exibindo o gráfico
plt.show()

"""Observam como alguns bairros até então fora do radar passam a aparecer nessa visualização. Provavelmente existe nesses bairros, pro tipo de acomodação que se destacou, alguma ocorrência com alta taxa de reviews, mas que só ficou evidente nesse tipo de visualização.

Na seção anterior, de análises unidimensionais, vimos que a mediana para essa variável tem valor 7. Vejamos então os dois cenários que apareceram com mais destaque: 'Entire home/apt' no bairro de Moneró e 'Private Room' no bairro da Penha.
"""

# buscando ocorrências em Moneró
ds_treated[(ds_treated[NEIGHBOURHOOD_CLEANSED] == 'Moneró') & (ds_treated[ROOM_TYPE] == ROOMTYPE_ENTIRE)]

"""De fato existe apenas uma ocorrência para esse filtro e ela tem um valor bem alto para a variável, quando comparado com a mediana de todo o dataset. Isso faz com que a mediana calculada para essa célula seja o próprio valor estabelecido para essa ocorrência e ela fique em destaque.

Ao acessar a url da acomodação pude verificar que trata-se de um local bem próximo ao aeroporto internacional do Galeão então possivelmente é muito utilizada por pessoas que precisam fazer um pernoite para conexões de vôos.
"""

ds_treated[(ds_treated[NEIGHBOURHOOD_CLEANSED] == 'Penha') & (ds_treated[ROOM_TYPE] == ROOMTYPE_PRIVATE)]

"""O mesmo cenário acontece para essa acomodação do bairro da Penha. Apenas uma ocorrência e de valor bem alto quando comparado com a mediana de todo o dataset.

Ao acessar a url, pude verificar que trata-se também de uma acomodação que divulga proximidade com o aeroporto do Galeão ("Room for two people - close to the airport") e provavelmente está sendo usada pelo mesmo tipo de hóspede, que busca pernoite.

Farei agora apenas uma última análise nas 10 acomodações com maiores valores da variável que indica quantidade de reviews no último ano.
"""

ds_treated.nlargest(10, NUMBER_OF_REVIEWS_LTM)[[LISTING_URL, ROOM_TYPE, ACCOMMODATES, PRICE, NUMBER_OF_REVIEWS_LTM, REVIEWS_PER_MONTH, NEIGHBOURHOOD_CLEANSED]]

"""Notem que os valores de quantidade de reviews vão de 99 até o máximo de 133. Acessei as urls dessas 10 acomodações e percebi dois clusters claros, excetuando a acomodação de Barra da Guaratiba.

Um cluster com as acomodações dos bairros do Centro e da Glória, cuja justificativa para o alto número de reviews é a proximidade com o aeroporto Santos Dumont (além da grande oferta de meios de tranporte do centro da cidade).

Outro cluster com as acomodações de Jacarepaguá e Camorim, cuja justificativa é a proximidade com o centro de convenções Riocentro, local onde ocorre com bastante frequência muitas feiras e grandes eventos. Além disso, a região também recebe o RockInRio, shows na Jeunesse Arena e outros eventos no Parque Olímpico.

A ocorrência isolada de Barra de Guaratiba se destaca e entrou nessa lista, a meu ver, por ter um diferencial de contato com a natureza, pôr do sol e isolamento. Chama a atenção também por ser a mais cara entre as 10. É bem provável que moradores da própria cidade do Rio estejam buscando essa experiência.

## 7. Tratamento final do dataset

Nessa seção eu vou fazer as alterações finais de dados no dataset de modo a otimizá-lo para uso em algoritmos de machine learning.

Algumas ações serão feitas antes do holdout e outras depois, para evitar data leakage.

A partir desse ponto não pretendo mais identificar as acomodações para qualquer análise específica no conteúdo da url então vou remover a variável listing_url.

Criarei uma nova cópia do dataset para operar com ele nessa seção.
"""

# criação de um novo dataset para executar as operações a partir da seção de holdout
# recuperando os nomes das colunas
col_treated = list(ds_treated.columns)

# o novo dataset irá conter todas as colunas do dataset original
ds_final = ds_treated[col_treated[:]]

# removendo a variável listing_url pois não será mais utilizada
ds_final.drop([LISTING_URL], axis=1, inplace= True)

"""Como a variável alvo tem valor numérico, técnicas de regressão serão utilizadas e por isso, o dataset deve ser adaptado para melhor se adequar a esse cenário de uso.

Vou adotar as seguintes ações de acordo com os tipos de dados:

*   manter dados do tipo booleano sem alteração;
*   transformar os dados que representam datas para melhor se adequar ao que eles representam para o problema;
*   usar OrdinalEncoder nas variáveis categóricas que tem ordenação;
*   usar dummy encoding nas variáveis categóricas sem ordenação;
*   padronizar variáveis numéricas cujas distribuições sejam normais;
*   normalizar variáveis numéricas cujas distribuições não sejam normais;

Inicio trazendo a informação sobre os tipos de dados de cada variável:

"""

# tipos de dados presentes no dataset
ds_final.dtypes

"""### 7.1 Manter dados booleanos

As variáveis isntant_bookable, has_neighbourhood_overview, host_is_superhost, host_identity_verified e has_host_about são booleanas e ficam de fora das ações de transformação pois os algoritmos de machine learning lidam bem com esse tipo de dado.

### 7.2 Transformar datas

As variáveis last_review e host_since são aquelas cujo tipo de dado é datetime. Mantive essas variáveis no dataset nas seções anteriores dadas as seguintes hipóteses:

*   last_review pode influenciar positivamente na taxa de ocupação na medida em que avaliações mais recentes tem mais credibilidade do que avaliações mais antigas. Essa hipótese portanto considera que quanto mais antiga uma avaliação, menos relevante ela fica para a taxa de ocupação.
*   host_since pode influenciar positivamente na taxa de ocupação na medida em que o anfitrião ganha experiência com o tempo de administração da acomodação. Essa hipótese considera portanto que anfitriões registrados a mais tempo tem um desempenho melhor e consequentemente a taxa de ocupação é melhor.

Ambas as hipóteses consideram a distância de tempo entre o momento atual e a data presente no dataset. No caso da primeira variável, quanto maior esse tempo, pior seria para o resultado. No caso da segunda variável, quanto maior o tempo calculado, melhor para o resultado.

Como esse dataset foi capturado no dia 22 de setembro de 2023 (ver seção de captura de dados), eu vou usar como data de referência o dia seguinte, 23 de setembro.

Desse modo, farei uma transformação nessas variáveis passando a adotar a quantidade de dias entre o valor da variável na instância e essa data de referência.
"""

# data de referência
data_referencia = pd.to_datetime('2023-09-23', format='%Y-%m-%d')

#calculo a diferença em quantidade de dias para last_review e substituo o valor original
ds_final[LAST_REVIEW] = (data_referencia - ds_final[LAST_REVIEW]).dt.days

#calculo a diferença em quantidade de dias para last_review e substituo o valor original
ds_final[HOST_SINCE] = (data_referencia - ds_final[HOST_SINCE]).dt.days

# Histogramas do dado host_since
ds_final[HOST_SINCE].hist(figsize = (8,3))
plt.show()

"""Notem que a distribuição da variável host_since segue aparentando uma distribuição normal, mesmo depois da transformação, como era esperado.

### 7.3 Aplicar Ordinal Encoder

A única variável categórica que de fato tem todas as categorias representando ordenação é a host_response_time.
"""

#valores únicos da categoria host_response_time
ds_final[HOST_RESPONSE_TIME].unique()

"""'within an hour' é menor do que 'within a few hours', que é menor do que 'within a day', que é menor do que 'a few days or more'.

Usarei o encoder que considera ordem pois se adequa melhor a esse caso.

"""

# Crie um objeto OrdinalEncoder
encoder = OrdinalEncoder(categories=[['within an hour', 'within a few hours', 'within a day', 'a few days or more']])

# Ajuste e transforme os dados
ds_final[HOST_RESPONSE_TIME] = encoder.fit_transform(ds_final[[HOST_RESPONSE_TIME]])

"""### 7.4 Aplicar Dummy Encoding

Vou aplicar dummy encoding à variável categórica que representa os bairros visto que são muitas opções diferentes.



"""

# vou transformar a variável de bairros
coluna_alvo = NEIGHBOURHOOD_CLEANSED

# vou extrair a coluna
coluna_categorica = ds_final[coluna_alvo]

# Aplico o Dummy Encoding à coluna escolhida
coluna_dummy = pd.get_dummies(coluna_categorica, prefix=coluna_alvo)

# Substituo a coluna original pelos valores dummy
ds_final = pd.concat([ds_final, coluna_dummy], axis=1)
ds_final = ds_final.drop(coluna_alvo, axis=1)

"""### 7.5 Aplicar One-Hot Encoding

Vou aplicar agora one-hot encoding nas variáveis categóricas restantes.
"""

# colunas para transformar usando One-Hot Encoding
colunas_para_one_hot = [ROOM_TYPE, BATHS_CATEGORY, HOST_PROXIMITY]

# Extraio as colunas escolhidas
colunas_categoricas = ds_final[colunas_para_one_hot]

# Crio um objeto OneHotEncoder
encoder = OneHotEncoder(sparse=False)

# Ajusto e transformo as colunas escolhidas
colunas_one_hot = encoder.fit_transform(colunas_categoricas)

# Crio um DataFrame com os resultados One-Hot Encoding e passo o índice original para garantir a correta concatenação na próxima etapa
colunas_one_hot_df = pd.DataFrame(colunas_one_hot, columns=encoder.get_feature_names_out(colunas_para_one_hot), index=colunas_categoricas.index)

# Substituo as colunas originais pelos valores One-Hot Encoding
ds_final = pd.concat([ds_final, colunas_one_hot_df], axis=1)

# removo as colunas originais usadas para onehot
ds_final = ds_final.drop(colunas_para_one_hot, axis=1)

"""### 7.6 Holdout

Vou usar uma divisão aleatória simples pois minha variável alvo é numérica.
"""

# separando em dois segmentos, o primeiro de variáveis independentes ou preditoras e o segundo sendo a variável dependente ou alvo
X = ds_final.drop(NUMBER_OF_REVIEWS_LTM, axis=1)  # X são todas as características, exceto number_of_reviews_ltm
y = ds_final[NUMBER_OF_REVIEWS_LTM]  # y é a variável alvo number_of_reviews_ltm

# Dividindo os dados em conjuntos de treino e teste (80% treino, 20% teste)
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)

"""### 7.7 Padronização

Voltando na seção de visualizações de dados, podemos notar que as variáveis amenities_count e host_since seguem uma distribuição normal ou uma aproximação dessa distribuição. Fiz inclusive uma transformação em host_since nesta seção de tratamento final mas vimos que isso não alterou a distribuição dos dados.

Vou aplicar então uma padronização nessas duas variáveis, no dataset de treino apenas, para evitar data leakage.
"""

# Colunas específicas a serem padronizadas
colunas_para_padronizar = [AMENITIES_COUNT, HOST_SINCE]

# Criar um objeto StandardScaler
scaler = StandardScaler()

# Aplicar a padronização apenas nas colunas selecionadas
X_treino[colunas_para_padronizar] = scaler.fit_transform(X_treino[colunas_para_padronizar])

#conferindo o resultado
X_treino[colunas_para_padronizar].head()

"""### 7.8 Normalização

Aplico agora a técnica de normalização nas variáveis restantes, que não apresentaram uma distribuição normal.
"""

# Normalização
colunas_para_normalizar = [ACCOMMODATES, BEDROOMS_ADPT, BEDS_ADPT, BATHS_INDEX, PRICE, MINIMUM_NIGHTS, MAXIMUM_NIGHTS, REVIEW_SCORES_RATING, NUMBER_OF_REVIEWS, LAST_REVIEW, REVIEWS_PER_MONTH, MONTHS_SINCE_REGISTERED, HOST_TOTAL_LISTINGS_COUNT, HOST_RESPONSE_RATE, HOST_ACCEPTANCE_RATE ]

# definindo o transformador como min max scaler
scaler = MinMaxScaler()

# Aplicar a normalização apenas nas colunas selecionadas
X_treino[colunas_para_normalizar] = scaler.fit_transform(X_treino[colunas_para_normalizar])

#conferindo o resultado
X_treino[colunas_para_normalizar].head()

X_treino.head()

# Commented out IPython magic to ensure Python compatibility.
!pip install pycodestyle pycodestyle_magic
!pip install flake8
# %load_ext pycodestyle_magic

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Criar um modelo de regressão linear
modelo_regressao_linear = LinearRegression()

# Treinar o modelo
modelo_regressao_linear.fit(X_treino, y_treino)

# Fazer previsões no conjunto de teste
previsoes = modelo_regressao_linear.predict(X_teste)

# Avaliar o desempenho do modelo
mse = mean_squared_error(y_teste, previsoes)
print(f'Mean Squared Error: {mse}')

rmse = mean_squared_error(y_teste, previsoes, squared=False)
print(f'Root Mean Squared Error: {rmse}')

r2 = r2_score(y_teste, previsoes)
print(f'R-squared: {r2}')
# Exibir os coeficientes e intercepto do modelo
print('Coeficientes:', modelo_regressao_linear.coef_)
print('Intercepto:', modelo_regressao_linear.intercept_)

from sklearn.model_selection import cross_val_score
import numpy as np

# Supondo que "modelo_regressao_linear" é o seu modelo treinado
# e "X" e "y" são seus conjuntos de features e alvo
scores = cross_val_score(modelo_regressao_linear, X, y, scoring='neg_mean_squared_error', cv=5)
mse_medio = np.mean(-scores)

print(f'Mean Cross-Validated MSE: {mse_medio}')

# Restaurar a configuração padrão
pd.reset_option('display.max_columns')
pd.reset_option('display.max_rows')

"""## 8. Conclusão

Fiz esse trabalho como parte dos compromissos de avaliação da especialização em ciência de dados e analytics, da PUC-RJ. No entanto, muito mais do que buscar a nota necessária para a aprovação, eu procurei explorar as possibilidades oferecidas em um desafio de análise e pré-processamento de dados. Essa etapa é essencial e primordial num projeto de ciência de dados então todo conhecimento adquirido vai ser muito útil para desafios futuros. Logo abaixo eu enumero meus aprendizados e as conclusões pertinentes, organizados por tópicos.

### 8.1 Notebook colab

Inicio minhas anotações de conclusão destacando a plataforma **Google Colaboratory** (Colab), tremenda facilitadora da execução desse tipo de trabalho. Ainda não tenho conhecimento sobre outras soluções de notebook então não posso fazer uma comparação, mas destaco a facilidade e a simplicidade oferecidas pelo colab, que permitem que o aluno vá direto ao foco do trabalho, sem precisar se preocupar com configurações ou qualquer preparação de ambiente.

### 8.2 Programação em Python e orientação a objetos

Eu iniciei minha carreira trabalhando como programador Java e atingi uma boa proeficiência nessa área, conhecendo bem os conceitos de orientação a objetos e design patterns. Foi bom revisitar os conceitos, dessa vez com Python, e retomar a liberdade propiciada pelo conhecimento de programação.

Em determinado momento da implementação do notebook, eu optei por implementar classes e métodos seguindo os conceitos de POO. Enfrentei algumas dificuldades que vou reportar aqui.

Se você leu todo o meu notebook, viu que eu escolhi fazer uma modelagem conceitual usando as entidades Acomodação, Localização da Acomodação e Anfitrião. As minhas classes foram implementadas segundo essa modelagem conceitual. No meu código eu instanciava um objeto dessas classes a partir da leitura de uma linha do dataset. As colunas com dados relacionados à entidade Acomodação era lidas para que o objeto fosse instanciado e o mesmo racional era utilizado para as outras entidades. Ao precisar fazer um tratamento dos dados, bastava eu chamar um método do próprio objeto que ele saberia ler seus atributos privados e executar o tratamento.

Eu comecei a ter dificuldades quando optei por criar colunas novas e remover antigas, na medida em que fazia tratamento dos dados. A instanciação de um objeto esperava a existência daquela coluna antiga que tinha sido deletada então passei a encarar um cenário de inconsistência entre a estrutura declarada das classes e o estado do dataset, que ia mudando ao longo do notebook, conforme eu ia fazendo os tratamentos de dados.

Frente a esse desafio, eu preferi voltar para a abordagem procedural do notebook, abrindo mão de uma implementação orientada a objetos, pois queria voltar a focar nos desafios de análise e pré-processamento de dados propriamente ditos.

### 8.3 Limpeza, tratamento e transformação de dados

Essa etapa fez jus à fama e foi bem trabalhosa. Na prática ela foi uma etapa que se extendeu por todo o notebook. Iniciei fazendo *feature selection*, vestindo o chapéu de especialista do domínio. Procurei simplificar bastante o dataset, eliminando colunas óbvias e outras nem tanto. Colunas e também linhas foram sendo eliminadas do dataset ao longo de todo o notebook, conforme mais conhecimento ia sendo adquirido sobre o conjunto de dados.

A organização dos dados na modelagem conceitual baseada em entidades ajudou a definir escopo e relevância para os dados disponíveis. Executei ações de *feature engineering*, criando novos dados a partir dos já existentes, ora para complementar dados ausentes, ora para de fato criar leituras e visões diferenciadas.

Tive dúvidas em algumas etapas de tratamento de dados, principalmente relacionadas à eficiência e utilidade para o uso nos algoritmos de machine learning mais pra frente. Foi muito bom praticar e implementar iniciativas variadas de tratamento pois gerou bastante aprendizado. No entanto, depois me peguei me perguntando se algumas das iniciativas de fato ajudam os algoritmos de ML.

Comento aqui um exemplo. Diversas ocorrências presentes no dataset não tem o dado de quantidade de quartos. Eu optei por preencher esse valor com 1 para todos os tipos de propriedade, exceto aquelas do tipo quarto compartilhado. Para estas, eu preenchi a variável com um valor fracionado, onde dividi 1 pela quantidade de hóspedes aceita. O resultado dessa divisão foi usado para preencher a variável nula. O raciocínio parece fazer sentido mas depois fiquei na dúvida se não seria melhor preencher também para os casos de quarto compartilhado o valor 1. No final das contas, existe de fato apenas 1 quarto e a informação de que aquela acomodação está sendo compartilhada viria de outra variável, e não dessa numérica. Li brevemente sobre a característica de multicolinearidade e como isso afeta algoritmos de regressão então fiquei preocupado da minha estratégia estar aumentando essa característica. Optei por manter o notebook com esse tratamento pois ainda não adquiri a experiência e o conhecimento dos algoritmos de machine learning (será no próximo módulo) para fazer a melhor avaliação.

Enfrentei uma dificuldade também na etapa de OneHot Encoding, para aplicar a transformação em colunas selecionadas, e depois introduzir o resultado no dataset original. Aconteceram questões relacionadas aos índices que fizeram meu dataset aumentar em tamanho de linhas, o que não era esperado. Com algumas pesquisas a mais eu descobri como resolver o problema.


### 8.4 Visualização de dados

Na etapa de visualização de dados, organizei a seção iniciando por gráficos de observação de uma única variável, passando para gráficos que relacionam duas variáveis e finalizei com os gráficos que relacionam 3 variáveis.

Essa foi outra seção em que optei por explorar muito alternativas diferentes. Iniciei com uma visão geral e me aprofundei na visualização de variáveis selecionadas. Foi super interessante ter percepções e insights que só são propiciados pelas soluções de visualização. Destaco aqui algumas:

*   perceber uma distribuição normal da variável accommodates, quando os valores são observados de maneira separada entre par e ímpar;
*   identificar os picos de cadastramento de anfitriões nos anos de 2014 e 2016 pelo gráfico de densidade, respectivamente anos de Copa do Mundo e Olimpíadas;
*   fazer uma melhor leitura da relação entre acomodações e anfitriões usando um gráfico de dispersão (scatter plot);
*   enxergar as manchas positivas e negativas na matriz de correlação;
*   comprovar ou refutar hipóteses de impacto na variável alvo com os gráficos de pequenos multíplos de boxplot;
*   perceber a diferença de correlação linear entre duas variáveis numéricas quando observadas em separado, por categoria de acomodação, ao fazer a leitura visual;
*   enxergar os cenários fora do padrão, nas visualizações por heatmap, com paletas de cores variadas. O caso do bairro Tijuca, foi bem característico de um insight por percepção visual.

Quando estava implementando a seção de heatmaps, me aprofundei mais no tema de paleta de cores. Foi muito interessante estudar os diferentes tipos e entender quando usar cada um deles. Até implementei uma paleta própria, associando uma variável presente no dataset com um tema que conheço por experiência profissional. O tema de paletas foi um aprendizado extra, decorrente da exploração que me propuz a fazer nesse trabalho.


### 8.5 Pair programming com chatGPT

O ano de 2023 foi marcado pela popularização das soluções baseadas em inteligência artificial generativas. Essas soluções estão possibilitando um incremento significativo na capacidade das pessoas em gerar conteúdos, sejam eles texto, imagens ou outros tipos de dados.

Na implementação desse trabalho, fiz bastante uso do chatGPT 3.5, no intuito de encurtar caminhos e buscar eficiência. Pela minha experiência profissional e por ter estudado bem o conteúdo do curso na plataforma online da PUC, eu conhecia bastante os conceitos e sabia muito bem o que eu queria fazer. Mas em diversas ocasiões deste trabalho, seria simplesmente mais rápido eu perguntar para a IA como fazer algo e obter a resposta, do que entrar no material para procurar ou pesquisar usando mecanismos de busca. Nem sempre essa estratégia funcionou bem e em algumas situações a IA entregou respostas erradas, com código que dava erro, ou com alguma outra inconsistência. Meu conhecimento de tecnologia foi essencial para avaliar essas respostas e saber quando descartá-las ou quando aproveitá-las fazendo as devidas modificações.

## 9. Pontos em aberto e melhorias

Criei essa seção para enumerar pontos em aberto e possibilidades de melhoria e evolução do trabalho. Seguem algumas sugestões:

*   o notebook não está generalizado a ponto de permitir a análise de acomodações de outras cidades diferentes do Rio de Janeiro. Existem trechos de código e trechos dos comentários em texto que mencionam valores específicos do Rio de Janeiro. Uma grande melhoria nesse notebook seria a generalização dele para que pudesse ser executado com um dataset de acomodações de outras cidades;
*   comentei na seção de conclusão sobre os desafios que enfrentei ao tentar fazer o código orientado a objetos. Uma melhoria para o notebook seria retomar essa abordagem, tratando os cenários de dificuldade que mencionei;
*   as técnicas de pré-processamento e tratamento dos dados que usei nesse notebook podem não ter sido as melhores escolhas para preparar o dataset para determinados algoritmos de machine learning. Uma pessoa que já saiba com clareza qual algoritmo vai usar e que tenha experiência no tema pode melhorar e incrementar essas técnicas de modo a atender necessidades de um algoritmo em específico.
*   o notebook tem alguns trechos de código escritos em inglês mas na sua grande parte está escrito em português. A tradução dele para inglês ampliaria a capacidade de análise e uso do mesmo para um público mais amplo.

## 10. Anexos

### Paleta de cores

Imagem obtida no site: https://www.codecademy.com/article/seaborn-design-ii

![color-palettes.webp](data:image/webp;base64,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)
"""